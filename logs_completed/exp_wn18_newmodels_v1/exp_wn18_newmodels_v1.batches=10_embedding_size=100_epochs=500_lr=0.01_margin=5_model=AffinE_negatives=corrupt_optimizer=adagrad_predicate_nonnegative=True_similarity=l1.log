Using Theano backend.
INFO:root:Acquiring data/wn18/wordnet-mlj12-train.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-valid.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-test.txt ..
INFO:root:Samples: 141442, no. batches: 10 -> batch size: 14145
INFO:root:Epoch no. 1 of 500 (samples: 141442)
INFO:root:Loss: 3.1979 +/- 0.0804
INFO:root:Epoch no. 2 of 500 (samples: 141442)
INFO:root:Loss: 2.8866 +/- 0.0488
INFO:root:Epoch no. 3 of 500 (samples: 141442)
INFO:root:Loss: 2.6168 +/- 0.042
INFO:root:Epoch no. 4 of 500 (samples: 141442)
INFO:root:Loss: 2.3809 +/- 0.028
INFO:root:Epoch no. 5 of 500 (samples: 141442)
INFO:root:Loss: 2.1926 +/- 0.0273
INFO:root:Epoch no. 6 of 500 (samples: 141442)
INFO:root:Loss: 2.0377 +/- 0.0174
INFO:root:Epoch no. 7 of 500 (samples: 141442)
INFO:root:Loss: 1.9039 +/- 0.013
INFO:root:Epoch no. 8 of 500 (samples: 141442)
INFO:root:Loss: 1.7902 +/- 0.0133
INFO:root:Epoch no. 9 of 500 (samples: 141442)
INFO:root:Loss: 1.6962 +/- 0.0103
INFO:root:Epoch no. 10 of 500 (samples: 141442)
INFO:root:Loss: 1.6065 +/- 0.0115
INFO:root:Epoch no. 11 of 500 (samples: 141442)
INFO:root:Loss: 1.5388 +/- 0.0077
INFO:root:Epoch no. 12 of 500 (samples: 141442)
INFO:root:Loss: 1.4718 +/- 0.0081
INFO:root:Epoch no. 13 of 500 (samples: 141442)
INFO:root:Loss: 1.4163 +/- 0.0117
INFO:root:Epoch no. 14 of 500 (samples: 141442)
INFO:root:Loss: 1.3671 +/- 0.0084
INFO:root:Epoch no. 15 of 500 (samples: 141442)
INFO:root:Loss: 1.3173 +/- 0.0082
INFO:root:Epoch no. 16 of 500 (samples: 141442)
INFO:root:Loss: 1.272 +/- 0.0086
INFO:root:Epoch no. 17 of 500 (samples: 141442)
INFO:root:Loss: 1.2343 +/- 0.0109
INFO:root:Epoch no. 18 of 500 (samples: 141442)
INFO:root:Loss: 1.197 +/- 0.0079
INFO:root:Epoch no. 19 of 500 (samples: 141442)
INFO:root:Loss: 1.1659 +/- 0.0043
INFO:root:Epoch no. 20 of 500 (samples: 141442)
INFO:root:Loss: 1.1341 +/- 0.0069
INFO:root:Epoch no. 21 of 500 (samples: 141442)
INFO:root:Loss: 1.103 +/- 0.0074
INFO:root:Epoch no. 22 of 500 (samples: 141442)
INFO:root:Loss: 1.0736 +/- 0.0067
INFO:root:Epoch no. 23 of 500 (samples: 141442)
INFO:root:Loss: 1.0498 +/- 0.0073
INFO:root:Epoch no. 24 of 500 (samples: 141442)
INFO:root:Loss: 1.0233 +/- 0.0112
INFO:root:Epoch no. 25 of 500 (samples: 141442)
INFO:root:Loss: 1.0015 +/- 0.0091
INFO:root:Epoch no. 26 of 500 (samples: 141442)
INFO:root:Loss: 0.9804 +/- 0.0071
INFO:root:Epoch no. 27 of 500 (samples: 141442)
INFO:root:Loss: 0.9614 +/- 0.0084
INFO:root:Epoch no. 28 of 500 (samples: 141442)
INFO:root:Loss: 0.9422 +/- 0.0063
INFO:root:Epoch no. 29 of 500 (samples: 141442)
INFO:root:Loss: 0.9225 +/- 0.0071
INFO:root:Epoch no. 30 of 500 (samples: 141442)
INFO:root:Loss: 0.9069 +/- 0.0082
INFO:root:Epoch no. 31 of 500 (samples: 141442)
INFO:root:Loss: 0.8871 +/- 0.0079
INFO:root:Epoch no. 32 of 500 (samples: 141442)
INFO:root:Loss: 0.8744 +/- 0.0054
INFO:root:Epoch no. 33 of 500 (samples: 141442)
INFO:root:Loss: 0.8597 +/- 0.0087
INFO:root:Epoch no. 34 of 500 (samples: 141442)
INFO:root:Loss: 0.8436 +/- 0.009
INFO:root:Epoch no. 35 of 500 (samples: 141442)
INFO:root:Loss: 0.829 +/- 0.0062
INFO:root:Epoch no. 36 of 500 (samples: 141442)
INFO:root:Loss: 0.8166 +/- 0.0046
INFO:root:Epoch no. 37 of 500 (samples: 141442)
INFO:root:Loss: 0.8021 +/- 0.0066
INFO:root:Epoch no. 38 of 500 (samples: 141442)
INFO:root:Loss: 0.7887 +/- 0.0065
INFO:root:Epoch no. 39 of 500 (samples: 141442)
INFO:root:Loss: 0.7749 +/- 0.0088
INFO:root:Epoch no. 40 of 500 (samples: 141442)
INFO:root:Loss: 0.7634 +/- 0.0051
INFO:root:Epoch no. 41 of 500 (samples: 141442)
INFO:root:Loss: 0.7549 +/- 0.0027
INFO:root:Epoch no. 42 of 500 (samples: 141442)
INFO:root:Loss: 0.7422 +/- 0.006
INFO:root:Epoch no. 43 of 500 (samples: 141442)
INFO:root:Loss: 0.7344 +/- 0.0076
INFO:root:Epoch no. 44 of 500 (samples: 141442)
INFO:root:Loss: 0.7251 +/- 0.0052
INFO:root:Epoch no. 45 of 500 (samples: 141442)
INFO:root:Loss: 0.713 +/- 0.0077
INFO:root:Epoch no. 46 of 500 (samples: 141442)
INFO:root:Loss: 0.7052 +/- 0.0064
INFO:root:Epoch no. 47 of 500 (samples: 141442)
INFO:root:Loss: 0.6946 +/- 0.0067
INFO:root:Epoch no. 48 of 500 (samples: 141442)
INFO:root:Loss: 0.6829 +/- 0.0074
INFO:root:Epoch no. 49 of 500 (samples: 141442)
INFO:root:Loss: 0.6755 +/- 0.0046
INFO:root:Epoch no. 50 of 500 (samples: 141442)
INFO:root:Loss: 0.6692 +/- 0.0045
INFO:root:Epoch no. 51 of 500 (samples: 141442)
INFO:root:Loss: 0.6647 +/- 0.0065
INFO:root:Epoch no. 52 of 500 (samples: 141442)
INFO:root:Loss: 0.6531 +/- 0.0076
INFO:root:Epoch no. 53 of 500 (samples: 141442)
INFO:root:Loss: 0.6464 +/- 0.0072
INFO:root:Epoch no. 54 of 500 (samples: 141442)
INFO:root:Loss: 0.6373 +/- 0.0036
INFO:root:Epoch no. 55 of 500 (samples: 141442)
INFO:root:Loss: 0.636 +/- 0.0059
INFO:root:Epoch no. 56 of 500 (samples: 141442)
INFO:root:Loss: 0.6261 +/- 0.0049
INFO:root:Epoch no. 57 of 500 (samples: 141442)
INFO:root:Loss: 0.6182 +/- 0.006
INFO:root:Epoch no. 58 of 500 (samples: 141442)
INFO:root:Loss: 0.6113 +/- 0.0055
INFO:root:Epoch no. 59 of 500 (samples: 141442)
INFO:root:Loss: 0.6056 +/- 0.0041
INFO:root:Epoch no. 60 of 500 (samples: 141442)
INFO:root:Loss: 0.5976 +/- 0.0077
INFO:root:Epoch no. 61 of 500 (samples: 141442)
INFO:root:Loss: 0.5938 +/- 0.0069
INFO:root:Epoch no. 62 of 500 (samples: 141442)
INFO:root:Loss: 0.5877 +/- 0.0037
INFO:root:Epoch no. 63 of 500 (samples: 141442)
INFO:root:Loss: 0.5828 +/- 0.0024
INFO:root:Epoch no. 64 of 500 (samples: 141442)
Traceback (most recent call last):
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: alloc failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./bin/hyper-cli.py", line 324, in <module>
    main(sys.argv[1:])
  File "./bin/hyper-cli.py", line 293, in main
    visualize=is_visualize)
  File "/home/insight/.local/lib/python3.4/site-packages/hyper-0.0.1-py3.4.egg/hyper/learning/core.py", line 185, in pairwise_training
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/models.py", line 409, in fit
    sample_weight=sample_weight)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 1052, in fit
    callback_metrics=callback_metrics)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 790, in _fit_loop
    outs = f(ins_batch)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/backend/theano_backend.py", line 518, in __call__
    return self.function(*inputs)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 871, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/usr/local/lib/python3.4/dist-packages/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/usr/local/lib/python3.4/dist-packages/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: alloc failed
Apply node that caused the error: Alloc(TensorConstant{(1, 1) of 0.0}, Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}.0, Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}[(0, 0)].0)
Toposort index: 51
Inputs types: [TensorType(float32, (True, True)), TensorType(int64, scalar), TensorType(int64, scalar)]
Inputs shapes: [(1, 1), (), ()]
Inputs strides: [(4, 4), (), ()]
Inputs values: [array([[ 0.]], dtype=float32), array(42435), array(10100)]
Outputs clients: [[IncSubtensor{InplaceInc;::, :int64:}(Alloc.0, Reshape{2}.0, ScalarFromTensor.0)]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
