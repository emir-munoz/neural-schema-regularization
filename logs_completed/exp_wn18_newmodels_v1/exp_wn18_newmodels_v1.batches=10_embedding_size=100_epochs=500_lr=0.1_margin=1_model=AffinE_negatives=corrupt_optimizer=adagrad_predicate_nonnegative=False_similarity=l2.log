Using Theano backend.
INFO:root:Acquiring data/wn18/wordnet-mlj12-train.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-valid.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-test.txt ..
INFO:root:Samples: 141442, no. batches: 10 -> batch size: 14145
INFO:root:Epoch no. 1 of 500 (samples: 141442)
INFO:root:Loss: 0.5964 +/- 0.135
INFO:root:Epoch no. 2 of 500 (samples: 141442)
INFO:root:Loss: 0.3272 +/- 0.019
INFO:root:Epoch no. 3 of 500 (samples: 141442)
INFO:root:Loss: 0.2341 +/- 0.0026
INFO:root:Epoch no. 4 of 500 (samples: 141442)
INFO:root:Loss: 0.18 +/- 0.0033
INFO:root:Epoch no. 5 of 500 (samples: 141442)
INFO:root:Loss: 0.1469 +/- 0.0039
INFO:root:Epoch no. 6 of 500 (samples: 141442)
INFO:root:Loss: 0.1254 +/- 0.0037
INFO:root:Epoch no. 7 of 500 (samples: 141442)
INFO:root:Loss: 0.108 +/- 0.0044
INFO:root:Epoch no. 8 of 500 (samples: 141442)
INFO:root:Loss: 0.096 +/- 0.0039
INFO:root:Epoch no. 9 of 500 (samples: 141442)
INFO:root:Loss: 0.0857 +/- 0.004
INFO:root:Epoch no. 10 of 500 (samples: 141442)
INFO:root:Loss: 0.0783 +/- 0.0038
INFO:root:Epoch no. 11 of 500 (samples: 141442)
INFO:root:Loss: 0.0724 +/- 0.0035
INFO:root:Epoch no. 12 of 500 (samples: 141442)
INFO:root:Loss: 0.0668 +/- 0.0032
INFO:root:Epoch no. 13 of 500 (samples: 141442)
INFO:root:Loss: 0.0624 +/- 0.0027
INFO:root:Epoch no. 14 of 500 (samples: 141442)
INFO:root:Loss: 0.0589 +/- 0.0033
INFO:root:Epoch no. 15 of 500 (samples: 141442)
INFO:root:Loss: 0.0562 +/- 0.0029
INFO:root:Epoch no. 16 of 500 (samples: 141442)
INFO:root:Loss: 0.0529 +/- 0.0029
INFO:root:Epoch no. 17 of 500 (samples: 141442)
INFO:root:Loss: 0.0502 +/- 0.0027
INFO:root:Epoch no. 18 of 500 (samples: 141442)
INFO:root:Loss: 0.0479 +/- 0.0025
INFO:root:Epoch no. 19 of 500 (samples: 141442)
INFO:root:Loss: 0.0462 +/- 0.0023
INFO:root:Epoch no. 20 of 500 (samples: 141442)
INFO:root:Loss: 0.0449 +/- 0.0027
INFO:root:Epoch no. 21 of 500 (samples: 141442)
INFO:root:Loss: 0.0426 +/- 0.002
INFO:root:Epoch no. 22 of 500 (samples: 141442)
INFO:root:Loss: 0.0414 +/- 0.0023
INFO:root:Epoch no. 23 of 500 (samples: 141442)
INFO:root:Loss: 0.04 +/- 0.0024
INFO:root:Epoch no. 24 of 500 (samples: 141442)
INFO:root:Loss: 0.0385 +/- 0.0022
INFO:root:Epoch no. 25 of 500 (samples: 141442)
INFO:root:Loss: 0.0372 +/- 0.0021
INFO:root:Epoch no. 26 of 500 (samples: 141442)
INFO:root:Loss: 0.0362 +/- 0.0018
INFO:root:Epoch no. 27 of 500 (samples: 141442)
INFO:root:Loss: 0.0353 +/- 0.0018
INFO:root:Epoch no. 28 of 500 (samples: 141442)
INFO:root:Loss: 0.0345 +/- 0.0021
INFO:root:Epoch no. 29 of 500 (samples: 141442)
INFO:root:Loss: 0.0337 +/- 0.0018
INFO:root:Epoch no. 30 of 500 (samples: 141442)
INFO:root:Loss: 0.0333 +/- 0.0018
INFO:root:Epoch no. 31 of 500 (samples: 141442)
INFO:root:Loss: 0.0321 +/- 0.0017
INFO:root:Epoch no. 32 of 500 (samples: 141442)
INFO:root:Loss: 0.0312 +/- 0.0015
INFO:root:Epoch no. 33 of 500 (samples: 141442)
INFO:root:Loss: 0.031 +/- 0.0015
INFO:root:Epoch no. 34 of 500 (samples: 141442)
INFO:root:Loss: 0.03 +/- 0.0015
INFO:root:Epoch no. 35 of 500 (samples: 141442)
INFO:root:Loss: 0.0299 +/- 0.0015
INFO:root:Epoch no. 36 of 500 (samples: 141442)
INFO:root:Loss: 0.0291 +/- 0.0017
INFO:root:Epoch no. 37 of 500 (samples: 141442)
INFO:root:Loss: 0.0283 +/- 0.0014
INFO:root:Epoch no. 38 of 500 (samples: 141442)
INFO:root:Loss: 0.0279 +/- 0.0014
INFO:root:Epoch no. 39 of 500 (samples: 141442)
INFO:root:Loss: 0.0271 +/- 0.0016
INFO:root:Epoch no. 40 of 500 (samples: 141442)
INFO:root:Loss: 0.0269 +/- 0.0013
INFO:root:Epoch no. 41 of 500 (samples: 141442)
INFO:root:Loss: 0.0265 +/- 0.0012
INFO:root:Epoch no. 42 of 500 (samples: 141442)
INFO:root:Loss: 0.0261 +/- 0.0017
INFO:root:Epoch no. 43 of 500 (samples: 141442)
INFO:root:Loss: 0.0258 +/- 0.0014
INFO:root:Epoch no. 44 of 500 (samples: 141442)
INFO:root:Loss: 0.0256 +/- 0.0012
INFO:root:Epoch no. 45 of 500 (samples: 141442)
INFO:root:Loss: 0.025 +/- 0.0012
INFO:root:Epoch no. 46 of 500 (samples: 141442)
INFO:root:Loss: 0.0246 +/- 0.0013
INFO:root:Epoch no. 47 of 500 (samples: 141442)
INFO:root:Loss: 0.0244 +/- 0.0013
INFO:root:Epoch no. 48 of 500 (samples: 141442)
INFO:root:Loss: 0.0239 +/- 0.0013
INFO:root:Epoch no. 49 of 500 (samples: 141442)
INFO:root:Loss: 0.0235 +/- 0.0014
INFO:root:Epoch no. 50 of 500 (samples: 141442)
INFO:root:Loss: 0.0235 +/- 0.0011
INFO:root:Epoch no. 51 of 500 (samples: 141442)
INFO:root:Loss: 0.0232 +/- 0.0012
INFO:root:Epoch no. 52 of 500 (samples: 141442)
INFO:root:Loss: 0.0226 +/- 0.0013
INFO:root:Epoch no. 53 of 500 (samples: 141442)
INFO:root:Loss: 0.0225 +/- 0.0013
INFO:root:Epoch no. 54 of 500 (samples: 141442)
INFO:root:Loss: 0.0221 +/- 0.001
INFO:root:Epoch no. 55 of 500 (samples: 141442)
INFO:root:Loss: 0.0219 +/- 0.0011
INFO:root:Epoch no. 56 of 500 (samples: 141442)
INFO:root:Loss: 0.0217 +/- 0.0012
INFO:root:Epoch no. 57 of 500 (samples: 141442)
INFO:root:Loss: 0.0215 +/- 0.0011
INFO:root:Epoch no. 58 of 500 (samples: 141442)
INFO:root:Loss: 0.0212 +/- 0.0012
INFO:root:Epoch no. 59 of 500 (samples: 141442)
INFO:root:Loss: 0.0212 +/- 0.0012
INFO:root:Epoch no. 60 of 500 (samples: 141442)
INFO:root:Loss: 0.0207 +/- 0.0011
INFO:root:Epoch no. 61 of 500 (samples: 141442)
INFO:root:Loss: 0.0206 +/- 0.0011
INFO:root:Epoch no. 62 of 500 (samples: 141442)
INFO:root:Loss: 0.0204 +/- 0.0011
INFO:root:Epoch no. 63 of 500 (samples: 141442)
INFO:root:Loss: 0.0202 +/- 0.0011
INFO:root:Epoch no. 64 of 500 (samples: 141442)
INFO:root:Loss: 0.0201 +/- 0.001
INFO:root:Epoch no. 65 of 500 (samples: 141442)
INFO:root:Loss: 0.0198 +/- 0.0012
INFO:root:Epoch no. 66 of 500 (samples: 141442)
INFO:root:Loss: 0.0195 +/- 0.0008
INFO:root:Epoch no. 67 of 500 (samples: 141442)
INFO:root:Loss: 0.0194 +/- 0.0009
INFO:root:Epoch no. 68 of 500 (samples: 141442)
INFO:root:Loss: 0.0193 +/- 0.0009
INFO:root:Epoch no. 69 of 500 (samples: 141442)
INFO:root:Loss: 0.0193 +/- 0.0009
INFO:root:Epoch no. 70 of 500 (samples: 141442)
INFO:root:Loss: 0.0191 +/- 0.0008
INFO:root:Epoch no. 71 of 500 (samples: 141442)
INFO:root:Loss: 0.0188 +/- 0.0009
INFO:root:Epoch no. 72 of 500 (samples: 141442)
INFO:root:Loss: 0.0189 +/- 0.001
INFO:root:Epoch no. 73 of 500 (samples: 141442)
INFO:root:Loss: 0.0186 +/- 0.0008
INFO:root:Epoch no. 74 of 500 (samples: 141442)
INFO:root:Loss: 0.0184 +/- 0.0009
INFO:root:Epoch no. 75 of 500 (samples: 141442)
INFO:root:Loss: 0.0183 +/- 0.001
INFO:root:Epoch no. 76 of 500 (samples: 141442)
INFO:root:Loss: 0.0181 +/- 0.001
INFO:root:Epoch no. 77 of 500 (samples: 141442)
INFO:root:Loss: 0.018 +/- 0.0009
INFO:root:Epoch no. 78 of 500 (samples: 141442)
INFO:root:Loss: 0.0179 +/- 0.001
INFO:root:Epoch no. 79 of 500 (samples: 141442)
INFO:root:Loss: 0.0177 +/- 0.0009
INFO:root:Epoch no. 80 of 500 (samples: 141442)
INFO:root:Loss: 0.0175 +/- 0.001
INFO:root:Epoch no. 81 of 500 (samples: 141442)
INFO:root:Loss: 0.0175 +/- 0.001
INFO:root:Epoch no. 82 of 500 (samples: 141442)
INFO:root:Loss: 0.0173 +/- 0.001
INFO:root:Epoch no. 83 of 500 (samples: 141442)
INFO:root:Loss: 0.0171 +/- 0.0008
INFO:root:Epoch no. 84 of 500 (samples: 141442)
INFO:root:Loss: 0.0169 +/- 0.0009
INFO:root:Epoch no. 85 of 500 (samples: 141442)
INFO:root:Loss: 0.0171 +/- 0.0008
INFO:root:Epoch no. 86 of 500 (samples: 141442)
INFO:root:Loss: 0.017 +/- 0.0008
INFO:root:Epoch no. 87 of 500 (samples: 141442)
INFO:root:Loss: 0.0167 +/- 0.0011
INFO:root:Epoch no. 88 of 500 (samples: 141442)
INFO:root:Loss: 0.0168 +/- 0.001
INFO:root:Epoch no. 89 of 500 (samples: 141442)
INFO:root:Loss: 0.0166 +/- 0.001
INFO:root:Epoch no. 90 of 500 (samples: 141442)
Traceback (most recent call last):
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: alloc failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./bin/hyper-cli.py", line 324, in <module>
    main(sys.argv[1:])
  File "./bin/hyper-cli.py", line 293, in main
    visualize=is_visualize)
  File "/home/insight/.local/lib/python3.4/site-packages/hyper-0.0.1-py3.4.egg/hyper/learning/core.py", line 185, in pairwise_training
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/models.py", line 409, in fit
    sample_weight=sample_weight)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 1052, in fit
    callback_metrics=callback_metrics)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 790, in _fit_loop
    outs = f(ins_batch)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/backend/theano_backend.py", line 518, in __call__
    return self.function(*inputs)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 871, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/usr/local/lib/python3.4/dist-packages/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/usr/local/lib/python3.4/dist-packages/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: alloc failed
Apply node that caused the error: Alloc(TensorConstant{(1, 1, 1) of 0.0}, Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}.0, Elemwise{Composite{Switch(EQ(i0, i1), ((i2 * i3 * i0) // (-(i3 * i0 * i2))), i0)}}.0, Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}[(0, 0)].0)
Toposort index: 50
Inputs types: [TensorType(float32, (True, True, True)), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]
Inputs shapes: [(1, 1, 1), (), (), ()]
Inputs strides: [(4, 4, 4), (), (), ()]
Inputs values: [array([[[ 0.]]], dtype=float32), array(42435), array(1), array(10100)]
Outputs clients: [[IncSubtensor{InplaceInc;::, int64, ::}(Alloc.0, IncSubtensor{InplaceInc;::, int64::}.0, Constant{0})]]

Backtrace when the node is created(use Theano flag traceback.limit=N to make it longer):
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1113, in access_term_cache
    input_grads = node.op.grad(inputs, new_output_grads)

HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
