Using Theano backend.
INFO:root:Acquiring data/wn18/wordnet-mlj12-train.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-valid.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-test.txt ..
INFO:root:Samples: 141442, no. batches: 10 -> batch size: 14145
INFO:root:Epoch no. 1 of 500 (samples: 141442)
INFO:root:Loss: 3.3005 +/- 0.0708
INFO:root:Epoch no. 2 of 500 (samples: 141442)
INFO:root:Loss: 3.2262 +/- 0.0045
INFO:root:Epoch no. 3 of 500 (samples: 141442)
INFO:root:Loss: 3.1832 +/- 0.003
INFO:root:Epoch no. 4 of 500 (samples: 141442)
INFO:root:Loss: 3.148 +/- 0.0039
INFO:root:Epoch no. 5 of 500 (samples: 141442)
INFO:root:Loss: 3.1149 +/- 0.0044
INFO:root:Epoch no. 6 of 500 (samples: 141442)
INFO:root:Loss: 3.0817 +/- 0.0053
INFO:root:Epoch no. 7 of 500 (samples: 141442)
INFO:root:Loss: 3.045 +/- 0.0047
INFO:root:Epoch no. 8 of 500 (samples: 141442)
INFO:root:Loss: 3.0062 +/- 0.0064
INFO:root:Epoch no. 9 of 500 (samples: 141442)
INFO:root:Loss: 2.9621 +/- 0.0053
INFO:root:Epoch no. 10 of 500 (samples: 141442)
INFO:root:Loss: 2.9127 +/- 0.0082
INFO:root:Epoch no. 11 of 500 (samples: 141442)
INFO:root:Loss: 2.8606 +/- 0.0075
INFO:root:Epoch no. 12 of 500 (samples: 141442)
INFO:root:Loss: 2.8045 +/- 0.007
INFO:root:Epoch no. 13 of 500 (samples: 141442)
INFO:root:Loss: 2.7478 +/- 0.0104
INFO:root:Epoch no. 14 of 500 (samples: 141442)
INFO:root:Loss: 2.6936 +/- 0.0085
INFO:root:Epoch no. 15 of 500 (samples: 141442)
INFO:root:Loss: 2.6396 +/- 0.011
INFO:root:Epoch no. 16 of 500 (samples: 141442)
INFO:root:Loss: 2.5863 +/- 0.0065
INFO:root:Epoch no. 17 of 500 (samples: 141442)
INFO:root:Loss: 2.538 +/- 0.0077
INFO:root:Epoch no. 18 of 500 (samples: 141442)
INFO:root:Loss: 2.4945 +/- 0.0069
INFO:root:Epoch no. 19 of 500 (samples: 141442)
INFO:root:Loss: 2.4559 +/- 0.0049
INFO:root:Epoch no. 20 of 500 (samples: 141442)
INFO:root:Loss: 2.4161 +/- 0.0073
INFO:root:Epoch no. 21 of 500 (samples: 141442)
INFO:root:Loss: 2.3801 +/- 0.0049
INFO:root:Epoch no. 22 of 500 (samples: 141442)
INFO:root:Loss: 2.3459 +/- 0.0076
INFO:root:Epoch no. 23 of 500 (samples: 141442)
INFO:root:Loss: 2.3141 +/- 0.011
INFO:root:Epoch no. 24 of 500 (samples: 141442)
INFO:root:Loss: 2.2921 +/- 0.0064
INFO:root:Epoch no. 25 of 500 (samples: 141442)
INFO:root:Loss: 2.2634 +/- 0.0049
INFO:root:Epoch no. 26 of 500 (samples: 141442)
INFO:root:Loss: 2.2392 +/- 0.0026
INFO:root:Epoch no. 27 of 500 (samples: 141442)
INFO:root:Loss: 2.2151 +/- 0.0073
INFO:root:Epoch no. 28 of 500 (samples: 141442)
INFO:root:Loss: 2.1937 +/- 0.0116
INFO:root:Epoch no. 29 of 500 (samples: 141442)
INFO:root:Loss: 2.1743 +/- 0.008
INFO:root:Epoch no. 30 of 500 (samples: 141442)
INFO:root:Loss: 2.1576 +/- 0.0086
INFO:root:Epoch no. 31 of 500 (samples: 141442)
INFO:root:Loss: 2.137 +/- 0.0064
INFO:root:Epoch no. 32 of 500 (samples: 141442)
INFO:root:Loss: 2.1194 +/- 0.0052
INFO:root:Epoch no. 33 of 500 (samples: 141442)
INFO:root:Loss: 2.1055 +/- 0.0082
INFO:root:Epoch no. 34 of 500 (samples: 141442)
INFO:root:Loss: 2.087 +/- 0.0058
INFO:root:Epoch no. 35 of 500 (samples: 141442)
INFO:root:Loss: 2.074 +/- 0.0076
INFO:root:Epoch no. 36 of 500 (samples: 141442)
INFO:root:Loss: 2.0617 +/- 0.0111
INFO:root:Epoch no. 37 of 500 (samples: 141442)
INFO:root:Loss: 2.0453 +/- 0.0079
INFO:root:Epoch no. 38 of 500 (samples: 141442)
INFO:root:Loss: 2.0344 +/- 0.006
INFO:root:Epoch no. 39 of 500 (samples: 141442)
INFO:root:Loss: 2.0199 +/- 0.0109
INFO:root:Epoch no. 40 of 500 (samples: 141442)
INFO:root:Loss: 2.007 +/- 0.0077
INFO:root:Epoch no. 41 of 500 (samples: 141442)
INFO:root:Loss: 1.9975 +/- 0.0111
INFO:root:Epoch no. 42 of 500 (samples: 141442)
INFO:root:Loss: 1.9873 +/- 0.006
INFO:root:Epoch no. 43 of 500 (samples: 141442)
INFO:root:Loss: 1.9745 +/- 0.0062
INFO:root:Epoch no. 44 of 500 (samples: 141442)
INFO:root:Loss: 1.9628 +/- 0.0089
INFO:root:Epoch no. 45 of 500 (samples: 141442)
INFO:root:Loss: 1.957 +/- 0.0083
INFO:root:Epoch no. 46 of 500 (samples: 141442)
INFO:root:Loss: 1.9457 +/- 0.0054
INFO:root:Epoch no. 47 of 500 (samples: 141442)
INFO:root:Loss: 1.937 +/- 0.0073
INFO:root:Epoch no. 48 of 500 (samples: 141442)
INFO:root:Loss: 1.9207 +/- 0.0129
INFO:root:Epoch no. 49 of 500 (samples: 141442)
INFO:root:Loss: 1.9158 +/- 0.0066
INFO:root:Epoch no. 50 of 500 (samples: 141442)
INFO:root:Loss: 1.9088 +/- 0.0073
INFO:root:Epoch no. 51 of 500 (samples: 141442)
INFO:root:Loss: 1.8989 +/- 0.0077
INFO:root:Epoch no. 52 of 500 (samples: 141442)
INFO:root:Loss: 1.8863 +/- 0.012
INFO:root:Epoch no. 53 of 500 (samples: 141442)
INFO:root:Loss: 1.8792 +/- 0.0081
INFO:root:Epoch no. 54 of 500 (samples: 141442)
INFO:root:Loss: 1.8703 +/- 0.0076
INFO:root:Epoch no. 55 of 500 (samples: 141442)
INFO:root:Loss: 1.8637 +/- 0.005
INFO:root:Epoch no. 56 of 500 (samples: 141442)
INFO:root:Loss: 1.8528 +/- 0.0087
INFO:root:Epoch no. 57 of 500 (samples: 141442)
INFO:root:Loss: 1.8499 +/- 0.0072
INFO:root:Epoch no. 58 of 500 (samples: 141442)
INFO:root:Loss: 1.837 +/- 0.0078
INFO:root:Epoch no. 59 of 500 (samples: 141442)
INFO:root:Loss: 1.8337 +/- 0.0102
INFO:root:Epoch no. 60 of 500 (samples: 141442)
INFO:root:Loss: 1.8209 +/- 0.0096
INFO:root:Epoch no. 61 of 500 (samples: 141442)
INFO:root:Loss: 1.8134 +/- 0.0116
INFO:root:Epoch no. 62 of 500 (samples: 141442)
INFO:root:Loss: 1.8078 +/- 0.004
INFO:root:Epoch no. 63 of 500 (samples: 141442)
INFO:root:Loss: 1.7995 +/- 0.0088
INFO:root:Epoch no. 64 of 500 (samples: 141442)
INFO:root:Loss: 1.7967 +/- 0.0115
INFO:root:Epoch no. 65 of 500 (samples: 141442)
INFO:root:Loss: 1.7851 +/- 0.0068
INFO:root:Epoch no. 66 of 500 (samples: 141442)
INFO:root:Loss: 1.7763 +/- 0.0053
INFO:root:Epoch no. 67 of 500 (samples: 141442)
INFO:root:Loss: 1.7719 +/- 0.0118
INFO:root:Epoch no. 68 of 500 (samples: 141442)
INFO:root:Loss: 1.7673 +/- 0.0073
INFO:root:Epoch no. 69 of 500 (samples: 141442)
INFO:root:Loss: 1.7609 +/- 0.0085
INFO:root:Epoch no. 70 of 500 (samples: 141442)
INFO:root:Loss: 1.7545 +/- 0.008
INFO:root:Epoch no. 71 of 500 (samples: 141442)
INFO:root:Loss: 1.7441 +/- 0.0113
INFO:root:Epoch no. 72 of 500 (samples: 141442)
INFO:root:Loss: 1.7369 +/- 0.0068
INFO:root:Epoch no. 73 of 500 (samples: 141442)
INFO:root:Loss: 1.7312 +/- 0.0086
INFO:root:Epoch no. 74 of 500 (samples: 141442)
INFO:root:Loss: 1.729 +/- 0.0083
INFO:root:Epoch no. 75 of 500 (samples: 141442)
INFO:root:Loss: 1.7181 +/- 0.0087
INFO:root:Epoch no. 76 of 500 (samples: 141442)
INFO:root:Loss: 1.7131 +/- 0.0083
INFO:root:Epoch no. 77 of 500 (samples: 141442)
INFO:root:Loss: 1.7048 +/- 0.0073
INFO:root:Epoch no. 78 of 500 (samples: 141442)
INFO:root:Loss: 1.6966 +/- 0.0127
INFO:root:Epoch no. 79 of 500 (samples: 141442)
INFO:root:Loss: 1.6942 +/- 0.0105
INFO:root:Epoch no. 80 of 500 (samples: 141442)
INFO:root:Loss: 1.6904 +/- 0.009
INFO:root:Epoch no. 81 of 500 (samples: 141442)
INFO:root:Loss: 1.6811 +/- 0.0102
INFO:root:Epoch no. 82 of 500 (samples: 141442)
INFO:root:Loss: 1.6795 +/- 0.0089
INFO:root:Epoch no. 83 of 500 (samples: 141442)
INFO:root:Loss: 1.6708 +/- 0.0088
INFO:root:Epoch no. 84 of 500 (samples: 141442)
INFO:root:Loss: 1.662 +/- 0.004
INFO:root:Epoch no. 85 of 500 (samples: 141442)
INFO:root:Loss: 1.6568 +/- 0.0088
INFO:root:Epoch no. 86 of 500 (samples: 141442)
INFO:root:Loss: 1.6493 +/- 0.0076
INFO:root:Epoch no. 87 of 500 (samples: 141442)
INFO:root:Loss: 1.6484 +/- 0.0085
INFO:root:Epoch no. 88 of 500 (samples: 141442)
INFO:root:Loss: 1.639 +/- 0.0067
INFO:root:Epoch no. 89 of 500 (samples: 141442)
INFO:root:Loss: 1.6356 +/- 0.0037
INFO:root:Epoch no. 90 of 500 (samples: 141442)
INFO:root:Loss: 1.6318 +/- 0.0072
INFO:root:Epoch no. 91 of 500 (samples: 141442)
INFO:root:Loss: 1.6212 +/- 0.01
INFO:root:Epoch no. 92 of 500 (samples: 141442)
INFO:root:Loss: 1.6154 +/- 0.0046
INFO:root:Epoch no. 93 of 500 (samples: 141442)
INFO:root:Loss: 1.6121 +/- 0.0075
INFO:root:Epoch no. 94 of 500 (samples: 141442)
INFO:root:Loss: 1.6064 +/- 0.0071
INFO:root:Epoch no. 95 of 500 (samples: 141442)
INFO:root:Loss: 1.6056 +/- 0.0072
INFO:root:Epoch no. 96 of 500 (samples: 141442)
INFO:root:Loss: 1.5985 +/- 0.0096
INFO:root:Epoch no. 97 of 500 (samples: 141442)
INFO:root:Loss: 1.5912 +/- 0.0103
INFO:root:Epoch no. 98 of 500 (samples: 141442)
INFO:root:Loss: 1.5857 +/- 0.0086
INFO:root:Epoch no. 99 of 500 (samples: 141442)
INFO:root:Loss: 1.5793 +/- 0.0077
INFO:root:Epoch no. 100 of 500 (samples: 141442)
INFO:root:Loss: 1.5739 +/- 0.0094
INFO:root:Epoch no. 101 of 500 (samples: 141442)
INFO:root:Loss: 1.5732 +/- 0.0098
INFO:root:Epoch no. 102 of 500 (samples: 141442)
INFO:root:Loss: 1.5613 +/- 0.0102
INFO:root:Epoch no. 103 of 500 (samples: 141442)
INFO:root:Loss: 1.5607 +/- 0.0087
INFO:root:Epoch no. 104 of 500 (samples: 141442)
INFO:root:Loss: 1.5539 +/- 0.008
INFO:root:Epoch no. 105 of 500 (samples: 141442)
INFO:root:Loss: 1.5471 +/- 0.0086
INFO:root:Epoch no. 106 of 500 (samples: 141442)
INFO:root:Loss: 1.5449 +/- 0.011
INFO:root:Epoch no. 107 of 500 (samples: 141442)
INFO:root:Loss: 1.5388 +/- 0.01
INFO:root:Epoch no. 108 of 500 (samples: 141442)
INFO:root:Loss: 1.5353 +/- 0.0085
INFO:root:Epoch no. 109 of 500 (samples: 141442)
INFO:root:Loss: 1.5305 +/- 0.0073
INFO:root:Epoch no. 110 of 500 (samples: 141442)
INFO:root:Loss: 1.5242 +/- 0.0082
INFO:root:Epoch no. 111 of 500 (samples: 141442)
INFO:root:Loss: 1.5176 +/- 0.0075
INFO:root:Epoch no. 112 of 500 (samples: 141442)
INFO:root:Loss: 1.5172 +/- 0.0092
INFO:root:Epoch no. 113 of 500 (samples: 141442)
INFO:root:Loss: 1.5091 +/- 0.0064
INFO:root:Epoch no. 114 of 500 (samples: 141442)
INFO:root:Loss: 1.5061 +/- 0.0096
INFO:root:Epoch no. 115 of 500 (samples: 141442)
INFO:root:Loss: 1.5038 +/- 0.0095
INFO:root:Epoch no. 116 of 500 (samples: 141442)
INFO:root:Loss: 1.4974 +/- 0.009
INFO:root:Epoch no. 117 of 500 (samples: 141442)
INFO:root:Loss: 1.4923 +/- 0.0119
INFO:root:Epoch no. 118 of 500 (samples: 141442)
INFO:root:Loss: 1.4896 +/- 0.0053
INFO:root:Epoch no. 119 of 500 (samples: 141442)
INFO:root:Loss: 1.4839 +/- 0.0098
INFO:root:Epoch no. 120 of 500 (samples: 141442)
INFO:root:Loss: 1.4794 +/- 0.0077
INFO:root:Epoch no. 121 of 500 (samples: 141442)
INFO:root:Loss: 1.4758 +/- 0.0092
INFO:root:Epoch no. 122 of 500 (samples: 141442)
INFO:root:Loss: 1.4702 +/- 0.0081
INFO:root:Epoch no. 123 of 500 (samples: 141442)
INFO:root:Loss: 1.4661 +/- 0.0073
INFO:root:Epoch no. 124 of 500 (samples: 141442)
INFO:root:Loss: 1.4647 +/- 0.0095
INFO:root:Epoch no. 125 of 500 (samples: 141442)
INFO:root:Loss: 1.4567 +/- 0.0063
INFO:root:Epoch no. 126 of 500 (samples: 141442)
INFO:root:Loss: 1.4539 +/- 0.0075
INFO:root:Epoch no. 127 of 500 (samples: 141442)
INFO:root:Loss: 1.4512 +/- 0.0089
INFO:root:Epoch no. 128 of 500 (samples: 141442)
INFO:root:Loss: 1.447 +/- 0.0065
INFO:root:Epoch no. 129 of 500 (samples: 141442)
INFO:root:Loss: 1.4422 +/- 0.0074
INFO:root:Epoch no. 130 of 500 (samples: 141442)
INFO:root:Loss: 1.439 +/- 0.0082
INFO:root:Epoch no. 131 of 500 (samples: 141442)
INFO:root:Loss: 1.4317 +/- 0.0104
INFO:root:Epoch no. 132 of 500 (samples: 141442)
INFO:root:Loss: 1.4298 +/- 0.0102
INFO:root:Epoch no. 133 of 500 (samples: 141442)
INFO:root:Loss: 1.4292 +/- 0.0084
INFO:root:Epoch no. 134 of 500 (samples: 141442)
INFO:root:Loss: 1.4245 +/- 0.0095
INFO:root:Epoch no. 135 of 500 (samples: 141442)
INFO:root:Loss: 1.4165 +/- 0.0133
INFO:root:Epoch no. 136 of 500 (samples: 141442)
INFO:root:Loss: 1.4191 +/- 0.0092
INFO:root:Epoch no. 137 of 500 (samples: 141442)
INFO:root:Loss: 1.4116 +/- 0.0094
INFO:root:Epoch no. 138 of 500 (samples: 141442)
INFO:root:Loss: 1.404 +/- 0.0094
INFO:root:Epoch no. 139 of 500 (samples: 141442)
INFO:root:Loss: 1.4036 +/- 0.0064
INFO:root:Epoch no. 140 of 500 (samples: 141442)
INFO:root:Loss: 1.3954 +/- 0.0074
INFO:root:Epoch no. 141 of 500 (samples: 141442)
INFO:root:Loss: 1.3967 +/- 0.0091
INFO:root:Epoch no. 142 of 500 (samples: 141442)
INFO:root:Loss: 1.3899 +/- 0.0075
INFO:root:Epoch no. 143 of 500 (samples: 141442)
INFO:root:Loss: 1.3877 +/- 0.0103
INFO:root:Epoch no. 144 of 500 (samples: 141442)
INFO:root:Loss: 1.3864 +/- 0.0088
INFO:root:Epoch no. 145 of 500 (samples: 141442)
INFO:root:Loss: 1.3805 +/- 0.009
INFO:root:Epoch no. 146 of 500 (samples: 141442)
INFO:root:Loss: 1.3784 +/- 0.0072
INFO:root:Epoch no. 147 of 500 (samples: 141442)
INFO:root:Loss: 1.3753 +/- 0.0109
INFO:root:Epoch no. 148 of 500 (samples: 141442)
INFO:root:Loss: 1.3685 +/- 0.0097
INFO:root:Epoch no. 149 of 500 (samples: 141442)
INFO:root:Loss: 1.3658 +/- 0.0054
INFO:root:Epoch no. 150 of 500 (samples: 141442)
INFO:root:Loss: 1.364 +/- 0.0066
INFO:root:Epoch no. 151 of 500 (samples: 141442)
INFO:root:Loss: 1.3582 +/- 0.0074
INFO:root:Epoch no. 152 of 500 (samples: 141442)
INFO:root:Loss: 1.3556 +/- 0.01
INFO:root:Epoch no. 153 of 500 (samples: 141442)
INFO:root:Loss: 1.3506 +/- 0.0094
INFO:root:Epoch no. 154 of 500 (samples: 141442)
INFO:root:Loss: 1.3475 +/- 0.0096
INFO:root:Epoch no. 155 of 500 (samples: 141442)
INFO:root:Loss: 1.3442 +/- 0.0064
INFO:root:Epoch no. 156 of 500 (samples: 141442)
INFO:root:Loss: 1.3449 +/- 0.008
INFO:root:Epoch no. 157 of 500 (samples: 141442)
INFO:root:Loss: 1.3373 +/- 0.0085
INFO:root:Epoch no. 158 of 500 (samples: 141442)
INFO:root:Loss: 1.3358 +/- 0.0072
INFO:root:Epoch no. 159 of 500 (samples: 141442)
INFO:root:Loss: 1.3319 +/- 0.0036
INFO:root:Epoch no. 160 of 500 (samples: 141442)
INFO:root:Loss: 1.3294 +/- 0.0097
INFO:root:Epoch no. 161 of 500 (samples: 141442)
INFO:root:Loss: 1.3281 +/- 0.0132
INFO:root:Epoch no. 162 of 500 (samples: 141442)
INFO:root:Loss: 1.3209 +/- 0.0057
INFO:root:Epoch no. 163 of 500 (samples: 141442)
INFO:root:Loss: 1.3164 +/- 0.009
INFO:root:Epoch no. 164 of 500 (samples: 141442)
INFO:root:Loss: 1.3167 +/- 0.0084
INFO:root:Epoch no. 165 of 500 (samples: 141442)
INFO:root:Loss: 1.3161 +/- 0.0066
INFO:root:Epoch no. 166 of 500 (samples: 141442)
INFO:root:Loss: 1.3077 +/- 0.0089
INFO:root:Epoch no. 167 of 500 (samples: 141442)
INFO:root:Loss: 1.3093 +/- 0.0097
INFO:root:Epoch no. 168 of 500 (samples: 141442)
INFO:root:Loss: 1.3026 +/- 0.0084
INFO:root:Epoch no. 169 of 500 (samples: 141442)
INFO:root:Loss: 1.2995 +/- 0.0068
INFO:root:Epoch no. 170 of 500 (samples: 141442)
INFO:root:Loss: 1.2964 +/- 0.0104
INFO:root:Epoch no. 171 of 500 (samples: 141442)
INFO:root:Loss: 1.2949 +/- 0.0065
INFO:root:Epoch no. 172 of 500 (samples: 141442)
INFO:root:Loss: 1.2913 +/- 0.0066
INFO:root:Epoch no. 173 of 500 (samples: 141442)
INFO:root:Loss: 1.2878 +/- 0.007
INFO:root:Epoch no. 174 of 500 (samples: 141442)
INFO:root:Loss: 1.2858 +/- 0.0088
INFO:root:Epoch no. 175 of 500 (samples: 141442)
INFO:root:Loss: 1.2813 +/- 0.0061
INFO:root:Epoch no. 176 of 500 (samples: 141442)
INFO:root:Loss: 1.2791 +/- 0.0088
INFO:root:Epoch no. 177 of 500 (samples: 141442)
INFO:root:Loss: 1.2758 +/- 0.0078
INFO:root:Epoch no. 178 of 500 (samples: 141442)
INFO:root:Loss: 1.2741 +/- 0.0059
INFO:root:Epoch no. 179 of 500 (samples: 141442)
INFO:root:Loss: 1.2706 +/- 0.0071
INFO:root:Epoch no. 180 of 500 (samples: 141442)
INFO:root:Loss: 1.2685 +/- 0.005
INFO:root:Epoch no. 181 of 500 (samples: 141442)
INFO:root:Loss: 1.2676 +/- 0.0061
INFO:root:Epoch no. 182 of 500 (samples: 141442)
INFO:root:Loss: 1.2616 +/- 0.0073
INFO:root:Epoch no. 183 of 500 (samples: 141442)
INFO:root:Loss: 1.263 +/- 0.0096
INFO:root:Epoch no. 184 of 500 (samples: 141442)
INFO:root:Loss: 1.2548 +/- 0.007
INFO:root:Epoch no. 185 of 500 (samples: 141442)
INFO:root:Loss: 1.2553 +/- 0.0077
INFO:root:Epoch no. 186 of 500 (samples: 141442)
INFO:root:Loss: 1.247 +/- 0.0061
INFO:root:Epoch no. 187 of 500 (samples: 141442)
INFO:root:Loss: 1.2487 +/- 0.0055
INFO:root:Epoch no. 188 of 500 (samples: 141442)
INFO:root:Loss: 1.246 +/- 0.0073
INFO:root:Epoch no. 189 of 500 (samples: 141442)
INFO:root:Loss: 1.241 +/- 0.0063
INFO:root:Epoch no. 190 of 500 (samples: 141442)
INFO:root:Loss: 1.2399 +/- 0.008
INFO:root:Epoch no. 191 of 500 (samples: 141442)
INFO:root:Loss: 1.2383 +/- 0.0088
INFO:root:Epoch no. 192 of 500 (samples: 141442)
INFO:root:Loss: 1.236 +/- 0.0066
INFO:root:Epoch no. 193 of 500 (samples: 141442)
INFO:root:Loss: 1.233 +/- 0.0064
INFO:root:Epoch no. 194 of 500 (samples: 141442)
INFO:root:Loss: 1.2325 +/- 0.0066
INFO:root:Epoch no. 195 of 500 (samples: 141442)
INFO:root:Loss: 1.2248 +/- 0.009
INFO:root:Epoch no. 196 of 500 (samples: 141442)
INFO:root:Loss: 1.2224 +/- 0.0075
INFO:root:Epoch no. 197 of 500 (samples: 141442)
INFO:root:Loss: 1.2217 +/- 0.0073
INFO:root:Epoch no. 198 of 500 (samples: 141442)
INFO:root:Loss: 1.2206 +/- 0.0045
INFO:root:Epoch no. 199 of 500 (samples: 141442)
INFO:root:Loss: 1.2172 +/- 0.0092
INFO:root:Epoch no. 200 of 500 (samples: 141442)
INFO:root:Loss: 1.2162 +/- 0.0083
INFO:root:Epoch no. 201 of 500 (samples: 141442)
INFO:root:Loss: 1.2103 +/- 0.0096
INFO:root:Epoch no. 202 of 500 (samples: 141442)
INFO:root:Loss: 1.2085 +/- 0.0058
INFO:root:Epoch no. 203 of 500 (samples: 141442)
INFO:root:Loss: 1.2075 +/- 0.0061
INFO:root:Epoch no. 204 of 500 (samples: 141442)
INFO:root:Loss: 1.2068 +/- 0.0099
INFO:root:Epoch no. 205 of 500 (samples: 141442)
INFO:root:Loss: 1.2002 +/- 0.0095
INFO:root:Epoch no. 206 of 500 (samples: 141442)
INFO:root:Loss: 1.1983 +/- 0.0085
INFO:root:Epoch no. 207 of 500 (samples: 141442)
INFO:root:Loss: 1.1985 +/- 0.0087
INFO:root:Epoch no. 208 of 500 (samples: 141442)
INFO:root:Loss: 1.1963 +/- 0.0042
INFO:root:Epoch no. 209 of 500 (samples: 141442)
INFO:root:Loss: 1.1925 +/- 0.0066
INFO:root:Epoch no. 210 of 500 (samples: 141442)
INFO:root:Loss: 1.1929 +/- 0.0073
INFO:root:Epoch no. 211 of 500 (samples: 141442)
INFO:root:Loss: 1.1843 +/- 0.0074
INFO:root:Epoch no. 212 of 500 (samples: 141442)
INFO:root:Loss: 1.185 +/- 0.006
INFO:root:Epoch no. 213 of 500 (samples: 141442)
INFO:root:Loss: 1.1814 +/- 0.0095
INFO:root:Epoch no. 214 of 500 (samples: 141442)
INFO:root:Loss: 1.1828 +/- 0.0071
INFO:root:Epoch no. 215 of 500 (samples: 141442)
INFO:root:Loss: 1.1807 +/- 0.0069
INFO:root:Epoch no. 216 of 500 (samples: 141442)
INFO:root:Loss: 1.1776 +/- 0.0047
INFO:root:Epoch no. 217 of 500 (samples: 141442)
INFO:root:Loss: 1.1753 +/- 0.0107
INFO:root:Epoch no. 218 of 500 (samples: 141442)
INFO:root:Loss: 1.1693 +/- 0.0049
INFO:root:Epoch no. 219 of 500 (samples: 141442)
INFO:root:Loss: 1.1671 +/- 0.0069
INFO:root:Epoch no. 220 of 500 (samples: 141442)
INFO:root:Loss: 1.1665 +/- 0.0057
INFO:root:Epoch no. 221 of 500 (samples: 141442)
INFO:root:Loss: 1.1649 +/- 0.0071
INFO:root:Epoch no. 222 of 500 (samples: 141442)
INFO:root:Loss: 1.1614 +/- 0.0063
INFO:root:Epoch no. 223 of 500 (samples: 141442)
INFO:root:Loss: 1.157 +/- 0.0056
INFO:root:Epoch no. 224 of 500 (samples: 141442)
INFO:root:Loss: 1.1567 +/- 0.0115
INFO:root:Epoch no. 225 of 500 (samples: 141442)
INFO:root:Loss: 1.1537 +/- 0.0105
INFO:root:Epoch no. 226 of 500 (samples: 141442)
INFO:root:Loss: 1.1484 +/- 0.0068
INFO:root:Epoch no. 227 of 500 (samples: 141442)
INFO:root:Loss: 1.1527 +/- 0.007
INFO:root:Epoch no. 228 of 500 (samples: 141442)
INFO:root:Loss: 1.1498 +/- 0.01
INFO:root:Epoch no. 229 of 500 (samples: 141442)
INFO:root:Loss: 1.1462 +/- 0.0075
INFO:root:Epoch no. 230 of 500 (samples: 141442)
INFO:root:Loss: 1.144 +/- 0.0073
INFO:root:Epoch no. 231 of 500 (samples: 141442)
INFO:root:Loss: 1.1428 +/- 0.0049
INFO:root:Epoch no. 232 of 500 (samples: 141442)
INFO:root:Loss: 1.1411 +/- 0.0075
INFO:root:Epoch no. 233 of 500 (samples: 141442)
INFO:root:Loss: 1.1401 +/- 0.0052
INFO:root:Epoch no. 234 of 500 (samples: 141442)
INFO:root:Loss: 1.1342 +/- 0.0088
INFO:root:Epoch no. 235 of 500 (samples: 141442)
INFO:root:Loss: 1.1336 +/- 0.0059
INFO:root:Epoch no. 236 of 500 (samples: 141442)
INFO:root:Loss: 1.1287 +/- 0.0066
INFO:root:Epoch no. 237 of 500 (samples: 141442)
INFO:root:Loss: 1.1312 +/- 0.0088
INFO:root:Epoch no. 238 of 500 (samples: 141442)
INFO:root:Loss: 1.1257 +/- 0.0044
INFO:root:Epoch no. 239 of 500 (samples: 141442)
INFO:root:Loss: 1.1256 +/- 0.0083
INFO:root:Epoch no. 240 of 500 (samples: 141442)
INFO:root:Loss: 1.1212 +/- 0.0072
INFO:root:Epoch no. 241 of 500 (samples: 141442)
INFO:root:Loss: 1.1219 +/- 0.0038
INFO:root:Epoch no. 242 of 500 (samples: 141442)
INFO:root:Loss: 1.1175 +/- 0.0055
INFO:root:Epoch no. 243 of 500 (samples: 141442)
INFO:root:Loss: 1.1216 +/- 0.0052
INFO:root:Epoch no. 244 of 500 (samples: 141442)
INFO:root:Loss: 1.1168 +/- 0.0076
INFO:root:Epoch no. 245 of 500 (samples: 141442)
INFO:root:Loss: 1.1128 +/- 0.0071
INFO:root:Epoch no. 246 of 500 (samples: 141442)
INFO:root:Loss: 1.1103 +/- 0.0058
INFO:root:Epoch no. 247 of 500 (samples: 141442)
INFO:root:Loss: 1.1023 +/- 0.0057
INFO:root:Epoch no. 248 of 500 (samples: 141442)
INFO:root:Loss: 1.1047 +/- 0.0085
INFO:root:Epoch no. 249 of 500 (samples: 141442)
INFO:root:Loss: 1.1048 +/- 0.0042
INFO:root:Epoch no. 250 of 500 (samples: 141442)
INFO:root:Loss: 1.1017 +/- 0.0053
INFO:root:Epoch no. 251 of 500 (samples: 141442)
INFO:root:Loss: 1.1008 +/- 0.0083
INFO:root:Epoch no. 252 of 500 (samples: 141442)
INFO:root:Loss: 1.0988 +/- 0.0078
INFO:root:Epoch no. 253 of 500 (samples: 141442)
INFO:root:Loss: 1.0982 +/- 0.0074
INFO:root:Epoch no. 254 of 500 (samples: 141442)
INFO:root:Loss: 1.0957 +/- 0.0073
INFO:root:Epoch no. 255 of 500 (samples: 141442)
INFO:root:Loss: 1.0927 +/- 0.0054
INFO:root:Epoch no. 256 of 500 (samples: 141442)
INFO:root:Loss: 1.0911 +/- 0.0085
INFO:root:Epoch no. 257 of 500 (samples: 141442)
INFO:root:Loss: 1.0858 +/- 0.0071
INFO:root:Epoch no. 258 of 500 (samples: 141442)
INFO:root:Loss: 1.0874 +/- 0.0084
INFO:root:Epoch no. 259 of 500 (samples: 141442)
INFO:root:Loss: 1.0828 +/- 0.0075
INFO:root:Epoch no. 260 of 500 (samples: 141442)
INFO:root:Loss: 1.0839 +/- 0.006
INFO:root:Epoch no. 261 of 500 (samples: 141442)
INFO:root:Loss: 1.0813 +/- 0.0085
INFO:root:Epoch no. 262 of 500 (samples: 141442)
INFO:root:Loss: 1.0819 +/- 0.0096
INFO:root:Epoch no. 263 of 500 (samples: 141442)
INFO:root:Loss: 1.0777 +/- 0.0069
INFO:root:Epoch no. 264 of 500 (samples: 141442)
INFO:root:Loss: 1.0751 +/- 0.0048
INFO:root:Epoch no. 265 of 500 (samples: 141442)
INFO:root:Loss: 1.0736 +/- 0.0068
INFO:root:Epoch no. 266 of 500 (samples: 141442)
INFO:root:Loss: 1.0734 +/- 0.0088
INFO:root:Epoch no. 267 of 500 (samples: 141442)
INFO:root:Loss: 1.068 +/- 0.0075
INFO:root:Epoch no. 268 of 500 (samples: 141442)
INFO:root:Loss: 1.0658 +/- 0.0097
INFO:root:Epoch no. 269 of 500 (samples: 141442)
INFO:root:Loss: 1.0685 +/- 0.008
INFO:root:Epoch no. 270 of 500 (samples: 141442)
Traceback (most recent call last):
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: alloc failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./bin/hyper-cli.py", line 324, in <module>
    main(sys.argv[1:])
  File "./bin/hyper-cli.py", line 293, in main
    visualize=is_visualize)
  File "/home/insight/.local/lib/python3.4/site-packages/hyper-0.0.1-py3.4.egg/hyper/learning/core.py", line 185, in pairwise_training
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/models.py", line 409, in fit
    sample_weight=sample_weight)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 1052, in fit
    callback_metrics=callback_metrics)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 790, in _fit_loop
    outs = f(ins_batch)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/backend/theano_backend.py", line 518, in __call__
    return self.function(*inputs)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 871, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/usr/local/lib/python3.4/dist-packages/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/usr/local/lib/python3.4/dist-packages/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: alloc failed
Apply node that caused the error: Alloc(TensorConstant{(1, 1, 1) of 0.0}, Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}[(0, 2)].0, Elemwise{Composite{Switch(EQ(i0, i1), ((i2 * i3 * i0) // (-(i3 * i0 * i2))), i0)}}.0, Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}.0)
Toposort index: 41
Inputs types: [TensorType(float32, (True, True, True)), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]
Inputs shapes: [(1, 1, 1), (), (), ()]
Inputs strides: [(4, 4, 4), (), (), ()]
Inputs values: [array([[[ 0.]]], dtype=float32), array(42435), array(1), array(2500)]
Outputs clients: [[IncSubtensor{InplaceInc;::, int64, ::}(Alloc.0, Reshape{2}.0, Constant{0})]]

Backtrace when the node is created(use Theano flag traceback.limit=N to make it longer):
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1113, in access_term_cache
    input_grads = node.op.grad(inputs, new_output_grads)

HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
Error in sys.excepthook:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/apport_python_hook.py", line 63, in apport_excepthook
    from apport.fileutils import likely_packaged, get_recent_crashes
  File "/usr/lib/python3/dist-packages/apport/__init__.py", line 5, in <module>
    from apport.report import Report
  File "/usr/lib/python3/dist-packages/apport/report.py", line 30, in <module>
    import apport.fileutils
  File "/usr/lib/python3/dist-packages/apport/fileutils.py", line 23, in <module>
    from apport.packaging_impl import impl as packaging
  File "/usr/lib/python3/dist-packages/apport/packaging_impl.py", line 20, in <module>
    import apt
  File "/usr/lib/python3/dist-packages/apt/__init__.py", line 23, in <module>
    import apt_pkg
ImportError: libapt-pkg.so.4.12: failed to map segment from shared object: Cannot allocate memory

Original exception was:
Traceback (most recent call last):
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: alloc failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./bin/hyper-cli.py", line 324, in <module>
    main(sys.argv[1:])
  File "./bin/hyper-cli.py", line 293, in main
    visualize=is_visualize)
  File "/home/insight/.local/lib/python3.4/site-packages/hyper-0.0.1-py3.4.egg/hyper/learning/core.py", line 185, in pairwise_training
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/models.py", line 409, in fit
    sample_weight=sample_weight)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 1052, in fit
    callback_metrics=callback_metrics)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 790, in _fit_loop
    outs = f(ins_batch)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/backend/theano_backend.py", line 518, in __call__
    return self.function(*inputs)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 871, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/usr/local/lib/python3.4/dist-packages/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/usr/local/lib/python3.4/dist-packages/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: alloc failed
Apply node that caused the error: Alloc(TensorConstant{(1, 1, 1) of 0.0}, Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}[(0, 2)].0, Elemwise{Composite{Switch(EQ(i0, i1), ((i2 * i3 * i0) // (-(i3 * i0 * i2))), i0)}}.0, Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}.0)
Toposort index: 41
Inputs types: [TensorType(float32, (True, True, True)), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]
Inputs shapes: [(1, 1, 1), (), (), ()]
Inputs strides: [(4, 4, 4), (), (), ()]
Inputs values: [array([[[ 0.]]], dtype=float32), array(42435), array(1), array(2500)]
Outputs clients: [[IncSubtensor{InplaceInc;::, int64, ::}(Alloc.0, Reshape{2}.0, Constant{0})]]

Backtrace when the node is created(use Theano flag traceback.limit=N to make it longer):
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1113, in access_term_cache
    input_grads = node.op.grad(inputs, new_output_grads)

HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
