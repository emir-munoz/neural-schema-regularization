Using Theano backend.
INFO:root:Acquiring data/wn18/wordnet-mlj12-train.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-valid.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-test.txt ..
INFO:root:Samples: 141442, no. batches: 10 -> batch size: 14145
INFO:root:Epoch no. 1 of 500 (samples: 141442)
INFO:root:Loss: 1.3326 +/- 0.0007
INFO:root:Epoch no. 2 of 500 (samples: 141442)
INFO:root:Loss: 1.3242 +/- 0.0014
INFO:root:Epoch no. 3 of 500 (samples: 141442)
INFO:root:Loss: 1.3133 +/- 0.0025
INFO:root:Epoch no. 4 of 500 (samples: 141442)
INFO:root:Loss: 1.2981 +/- 0.0037
INFO:root:Epoch no. 5 of 500 (samples: 141442)
INFO:root:Loss: 1.277 +/- 0.0052
INFO:root:Epoch no. 6 of 500 (samples: 141442)
INFO:root:Loss: 1.2494 +/- 0.007
INFO:root:Epoch no. 7 of 500 (samples: 141442)
INFO:root:Loss: 1.2156 +/- 0.0077
INFO:root:Epoch no. 8 of 500 (samples: 141442)
INFO:root:Loss: 1.1802 +/- 0.0076
INFO:root:Epoch no. 9 of 500 (samples: 141442)
INFO:root:Loss: 1.1484 +/- 0.0058
INFO:root:Epoch no. 10 of 500 (samples: 141442)
INFO:root:Loss: 1.1203 +/- 0.0044
INFO:root:Epoch no. 11 of 500 (samples: 141442)
INFO:root:Loss: 1.0974 +/- 0.0032
INFO:root:Epoch no. 12 of 500 (samples: 141442)
INFO:root:Loss: 1.0768 +/- 0.0029
INFO:root:Epoch no. 13 of 500 (samples: 141442)
INFO:root:Loss: 1.0608 +/- 0.0028
INFO:root:Epoch no. 14 of 500 (samples: 141442)
INFO:root:Loss: 1.0483 +/- 0.0031
INFO:root:Epoch no. 15 of 500 (samples: 141442)
INFO:root:Loss: 1.0373 +/- 0.0028
INFO:root:Epoch no. 16 of 500 (samples: 141442)
INFO:root:Loss: 1.0256 +/- 0.0043
INFO:root:Epoch no. 17 of 500 (samples: 141442)
INFO:root:Loss: 1.0165 +/- 0.0028
INFO:root:Epoch no. 18 of 500 (samples: 141442)
INFO:root:Loss: 1.0105 +/- 0.0034
INFO:root:Epoch no. 19 of 500 (samples: 141442)
INFO:root:Loss: 1.0038 +/- 0.0021
INFO:root:Epoch no. 20 of 500 (samples: 141442)
INFO:root:Loss: 0.9979 +/- 0.0033
INFO:root:Epoch no. 21 of 500 (samples: 141442)
INFO:root:Loss: 0.9918 +/- 0.0014
INFO:root:Epoch no. 22 of 500 (samples: 141442)
INFO:root:Loss: 0.9871 +/- 0.0018
INFO:root:Epoch no. 23 of 500 (samples: 141442)
INFO:root:Loss: 0.9813 +/- 0.0034
INFO:root:Epoch no. 24 of 500 (samples: 141442)
INFO:root:Loss: 0.978 +/- 0.003
INFO:root:Epoch no. 25 of 500 (samples: 141442)
INFO:root:Loss: 0.9742 +/- 0.0032
INFO:root:Epoch no. 26 of 500 (samples: 141442)
INFO:root:Loss: 0.9684 +/- 0.0027
INFO:root:Epoch no. 27 of 500 (samples: 141442)
INFO:root:Loss: 0.9658 +/- 0.0028
INFO:root:Epoch no. 28 of 500 (samples: 141442)
INFO:root:Loss: 0.9619 +/- 0.0032
INFO:root:Epoch no. 29 of 500 (samples: 141442)
INFO:root:Loss: 0.9583 +/- 0.0034
INFO:root:Epoch no. 30 of 500 (samples: 141442)
INFO:root:Loss: 0.9541 +/- 0.0036
INFO:root:Epoch no. 31 of 500 (samples: 141442)
INFO:root:Loss: 0.9514 +/- 0.0022
INFO:root:Epoch no. 32 of 500 (samples: 141442)
INFO:root:Loss: 0.9469 +/- 0.0033
INFO:root:Epoch no. 33 of 500 (samples: 141442)
INFO:root:Loss: 0.9465 +/- 0.0036
INFO:root:Epoch no. 34 of 500 (samples: 141442)
INFO:root:Loss: 0.9421 +/- 0.0028
INFO:root:Epoch no. 35 of 500 (samples: 141442)
INFO:root:Loss: 0.9392 +/- 0.0037
INFO:root:Epoch no. 36 of 500 (samples: 141442)
INFO:root:Loss: 0.9351 +/- 0.0036
INFO:root:Epoch no. 37 of 500 (samples: 141442)
INFO:root:Loss: 0.9331 +/- 0.002
INFO:root:Epoch no. 38 of 500 (samples: 141442)
INFO:root:Loss: 0.9303 +/- 0.0027
INFO:root:Epoch no. 39 of 500 (samples: 141442)
INFO:root:Loss: 0.9274 +/- 0.0029
INFO:root:Epoch no. 40 of 500 (samples: 141442)
INFO:root:Loss: 0.9239 +/- 0.0025
INFO:root:Epoch no. 41 of 500 (samples: 141442)
INFO:root:Loss: 0.921 +/- 0.003
INFO:root:Epoch no. 42 of 500 (samples: 141442)
INFO:root:Loss: 0.9195 +/- 0.0022
INFO:root:Epoch no. 43 of 500 (samples: 141442)
INFO:root:Loss: 0.9174 +/- 0.0035
INFO:root:Epoch no. 44 of 500 (samples: 141442)
INFO:root:Loss: 0.9144 +/- 0.0028
INFO:root:Epoch no. 45 of 500 (samples: 141442)
INFO:root:Loss: 0.9121 +/- 0.0021
INFO:root:Epoch no. 46 of 500 (samples: 141442)
INFO:root:Loss: 0.9089 +/- 0.0024
INFO:root:Epoch no. 47 of 500 (samples: 141442)
INFO:root:Loss: 0.9062 +/- 0.0022
INFO:root:Epoch no. 48 of 500 (samples: 141442)
INFO:root:Loss: 0.9031 +/- 0.0054
INFO:root:Epoch no. 49 of 500 (samples: 141442)
INFO:root:Loss: 0.9007 +/- 0.004
INFO:root:Epoch no. 50 of 500 (samples: 141442)
INFO:root:Loss: 0.8985 +/- 0.004
INFO:root:Epoch no. 51 of 500 (samples: 141442)
INFO:root:Loss: 0.8955 +/- 0.0027
INFO:root:Epoch no. 52 of 500 (samples: 141442)
INFO:root:Loss: 0.8921 +/- 0.0048
INFO:root:Epoch no. 53 of 500 (samples: 141442)
INFO:root:Loss: 0.889 +/- 0.0048
INFO:root:Epoch no. 54 of 500 (samples: 141442)
INFO:root:Loss: 0.8882 +/- 0.0036
INFO:root:Epoch no. 55 of 500 (samples: 141442)
INFO:root:Loss: 0.8846 +/- 0.0019
INFO:root:Epoch no. 56 of 500 (samples: 141442)
INFO:root:Loss: 0.883 +/- 0.0027
INFO:root:Epoch no. 57 of 500 (samples: 141442)
INFO:root:Loss: 0.8809 +/- 0.0027
INFO:root:Epoch no. 58 of 500 (samples: 141442)
INFO:root:Loss: 0.8774 +/- 0.0033
INFO:root:Epoch no. 59 of 500 (samples: 141442)
INFO:root:Loss: 0.876 +/- 0.0053
INFO:root:Epoch no. 60 of 500 (samples: 141442)
INFO:root:Loss: 0.8729 +/- 0.0038
INFO:root:Epoch no. 61 of 500 (samples: 141442)
INFO:root:Loss: 0.8706 +/- 0.0041
INFO:root:Epoch no. 62 of 500 (samples: 141442)
INFO:root:Loss: 0.868 +/- 0.0035
INFO:root:Epoch no. 63 of 500 (samples: 141442)
INFO:root:Loss: 0.8658 +/- 0.0032
INFO:root:Epoch no. 64 of 500 (samples: 141442)
INFO:root:Loss: 0.8644 +/- 0.0031
INFO:root:Epoch no. 65 of 500 (samples: 141442)
INFO:root:Loss: 0.8619 +/- 0.0021
INFO:root:Epoch no. 66 of 500 (samples: 141442)
INFO:root:Loss: 0.8579 +/- 0.0034
INFO:root:Epoch no. 67 of 500 (samples: 141442)
INFO:root:Loss: 0.858 +/- 0.0022
INFO:root:Epoch no. 68 of 500 (samples: 141442)
INFO:root:Loss: 0.8552 +/- 0.0044
INFO:root:Epoch no. 69 of 500 (samples: 141442)
INFO:root:Loss: 0.8536 +/- 0.0041
INFO:root:Epoch no. 70 of 500 (samples: 141442)
INFO:root:Loss: 0.8497 +/- 0.0015
INFO:root:Epoch no. 71 of 500 (samples: 141442)
INFO:root:Loss: 0.8472 +/- 0.0036
INFO:root:Epoch no. 72 of 500 (samples: 141442)
INFO:root:Loss: 0.8459 +/- 0.0037
INFO:root:Epoch no. 73 of 500 (samples: 141442)
INFO:root:Loss: 0.8451 +/- 0.0035
INFO:root:Epoch no. 74 of 500 (samples: 141442)
INFO:root:Loss: 0.8431 +/- 0.0039
INFO:root:Epoch no. 75 of 500 (samples: 141442)
INFO:root:Loss: 0.8409 +/- 0.0044
INFO:root:Epoch no. 76 of 500 (samples: 141442)
INFO:root:Loss: 0.8387 +/- 0.0036
INFO:root:Epoch no. 77 of 500 (samples: 141442)
INFO:root:Loss: 0.8359 +/- 0.0029
INFO:root:Epoch no. 78 of 500 (samples: 141442)
INFO:root:Loss: 0.8334 +/- 0.0032
INFO:root:Epoch no. 79 of 500 (samples: 141442)
INFO:root:Loss: 0.8327 +/- 0.0037
INFO:root:Epoch no. 80 of 500 (samples: 141442)
INFO:root:Loss: 0.8307 +/- 0.0033
INFO:root:Epoch no. 81 of 500 (samples: 141442)
INFO:root:Loss: 0.8288 +/- 0.0032
INFO:root:Epoch no. 82 of 500 (samples: 141442)
INFO:root:Loss: 0.827 +/- 0.0031
INFO:root:Epoch no. 83 of 500 (samples: 141442)
INFO:root:Loss: 0.8248 +/- 0.0037
INFO:root:Epoch no. 84 of 500 (samples: 141442)
INFO:root:Loss: 0.8217 +/- 0.0033
INFO:root:Epoch no. 85 of 500 (samples: 141442)
INFO:root:Loss: 0.8197 +/- 0.0042
INFO:root:Epoch no. 86 of 500 (samples: 141442)
INFO:root:Loss: 0.8188 +/- 0.0026
INFO:root:Epoch no. 87 of 500 (samples: 141442)
INFO:root:Loss: 0.818 +/- 0.0022
INFO:root:Epoch no. 88 of 500 (samples: 141442)
INFO:root:Loss: 0.8152 +/- 0.0031
INFO:root:Epoch no. 89 of 500 (samples: 141442)
INFO:root:Loss: 0.8141 +/- 0.0031
INFO:root:Epoch no. 90 of 500 (samples: 141442)
INFO:root:Loss: 0.8126 +/- 0.0044
INFO:root:Epoch no. 91 of 500 (samples: 141442)
INFO:root:Loss: 0.8105 +/- 0.0035
INFO:root:Epoch no. 92 of 500 (samples: 141442)
INFO:root:Loss: 0.808 +/- 0.0035
INFO:root:Epoch no. 93 of 500 (samples: 141442)
INFO:root:Loss: 0.8071 +/- 0.0031
INFO:root:Epoch no. 94 of 500 (samples: 141442)
INFO:root:Loss: 0.8047 +/- 0.0025
INFO:root:Epoch no. 95 of 500 (samples: 141442)
INFO:root:Loss: 0.804 +/- 0.0027
INFO:root:Epoch no. 96 of 500 (samples: 141442)
INFO:root:Loss: 0.8034 +/- 0.0033
INFO:root:Epoch no. 97 of 500 (samples: 141442)
INFO:root:Loss: 0.8001 +/- 0.0025
INFO:root:Epoch no. 98 of 500 (samples: 141442)
INFO:root:Loss: 0.7997 +/- 0.002
INFO:root:Epoch no. 99 of 500 (samples: 141442)
INFO:root:Loss: 0.7981 +/- 0.0028
INFO:root:Epoch no. 100 of 500 (samples: 141442)
INFO:root:Loss: 0.7953 +/- 0.0049
INFO:root:Epoch no. 101 of 500 (samples: 141442)
INFO:root:Loss: 0.7957 +/- 0.003
INFO:root:Epoch no. 102 of 500 (samples: 141442)
INFO:root:Loss: 0.794 +/- 0.0028
INFO:root:Epoch no. 103 of 500 (samples: 141442)
INFO:root:Loss: 0.7923 +/- 0.003
INFO:root:Epoch no. 104 of 500 (samples: 141442)
INFO:root:Loss: 0.7897 +/- 0.0031
INFO:root:Epoch no. 105 of 500 (samples: 141442)
INFO:root:Loss: 0.7885 +/- 0.003
INFO:root:Epoch no. 106 of 500 (samples: 141442)
INFO:root:Loss: 0.7873 +/- 0.0045
INFO:root:Epoch no. 107 of 500 (samples: 141442)
INFO:root:Loss: 0.7867 +/- 0.0032
INFO:root:Epoch no. 108 of 500 (samples: 141442)
INFO:root:Loss: 0.7863 +/- 0.0032
INFO:root:Epoch no. 109 of 500 (samples: 141442)
INFO:root:Loss: 0.7831 +/- 0.0037
INFO:root:Epoch no. 110 of 500 (samples: 141442)
INFO:root:Loss: 0.7829 +/- 0.0027
INFO:root:Epoch no. 111 of 500 (samples: 141442)
INFO:root:Loss: 0.7806 +/- 0.0032
INFO:root:Epoch no. 112 of 500 (samples: 141442)
INFO:root:Loss: 0.7796 +/- 0.0021
INFO:root:Epoch no. 113 of 500 (samples: 141442)
INFO:root:Loss: 0.778 +/- 0.0037
INFO:root:Epoch no. 114 of 500 (samples: 141442)
INFO:root:Loss: 0.7775 +/- 0.0032
INFO:root:Epoch no. 115 of 500 (samples: 141442)
INFO:root:Loss: 0.7761 +/- 0.0026
INFO:root:Epoch no. 116 of 500 (samples: 141442)
INFO:root:Loss: 0.7743 +/- 0.003
INFO:root:Epoch no. 117 of 500 (samples: 141442)
INFO:root:Loss: 0.7731 +/- 0.0034
INFO:root:Epoch no. 118 of 500 (samples: 141442)
INFO:root:Loss: 0.7733 +/- 0.0033
INFO:root:Epoch no. 119 of 500 (samples: 141442)
INFO:root:Loss: 0.7701 +/- 0.0029
INFO:root:Epoch no. 120 of 500 (samples: 141442)
INFO:root:Loss: 0.7711 +/- 0.0028
INFO:root:Epoch no. 121 of 500 (samples: 141442)
INFO:root:Loss: 0.769 +/- 0.0041
INFO:root:Epoch no. 122 of 500 (samples: 141442)
INFO:root:Loss: 0.7683 +/- 0.0036
INFO:root:Epoch no. 123 of 500 (samples: 141442)
INFO:root:Loss: 0.7676 +/- 0.0032
INFO:root:Epoch no. 124 of 500 (samples: 141442)
Traceback (most recent call last):
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./bin/hyper-cli.py", line 324, in <module>
    main(sys.argv[1:])
  File "./bin/hyper-cli.py", line 293, in main
    visualize=is_visualize)
  File "/home/insight/.local/lib/python3.4/site-packages/hyper-0.0.1-py3.4.egg/hyper/learning/core.py", line 185, in pairwise_training
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/models.py", line 409, in fit
    sample_weight=sample_weight)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 1052, in fit
    callback_metrics=callback_metrics)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 790, in _fit_loop
    outs = f(ins_batch)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/backend/theano_backend.py", line 518, in __call__
    return self.function(*inputs)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 871, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/usr/local/lib/python3.4/dist-packages/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/usr/local/lib/python3.4/dist-packages/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: 
Apply node that caused the error: AdvancedSubtensor1(embedding_1_W, Reshape{1}.0)
Toposort index: 19
Inputs types: [TensorType(float32, matrix), TensorType(int32, vector)]
Inputs shapes: [(19, 2500), (42435,)]
Inputs strides: [(10000, 4), (4,)]
Inputs values: ['not shown', 'not shown']
Outputs clients: [[Reshape{3}(AdvancedSubtensor1.0, MakeVector{dtype='int64'}.0)]]

Backtrace when the node is created(use Theano flag traceback.limit=N to make it longer):
  File "/home/insight/.local/lib/python3.4/site-packages/hyper-0.0.1-py3.4.egg/hyper/learning/core.py", line 56, in pairwise_training
    predicate_encoder.add(predicate_embedding_layer)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/models.py", line 114, in add
    layer.create_input_layer(batch_input_shape, input_dtype)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/topology.py", line 341, in create_input_layer
    self(x)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/topology.py", line 485, in __call__
    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/topology.py", line 543, in add_inbound_node
    Node.create_node(self, inbound_layers, node_indices, tensor_indices)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/topology.py", line 148, in create_node
    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/layers/embeddings.py", line 135, in call
    out = K.gather(W, x)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/backend/theano_backend.py", line 166, in gather
    return reference[indices]

HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
