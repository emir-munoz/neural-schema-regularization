Using Theano backend.
INFO:root:Acquiring data/wn18/wordnet-mlj12-train.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-valid.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-test.txt ..
INFO:root:Samples: 141442, no. batches: 10 -> batch size: 14145
INFO:root:Epoch no. 1 of 500 (samples: 141442)
INFO:root:Loss: 1.3282 +/- 0.0046
INFO:root:Epoch no. 2 of 500 (samples: 141442)
INFO:root:Loss: 1.2384 +/- 0.0433
INFO:root:Epoch no. 3 of 500 (samples: 141442)
INFO:root:Loss: 1.0103 +/- 0.0387
INFO:root:Epoch no. 4 of 500 (samples: 141442)
INFO:root:Loss: 0.8863 +/- 0.0082
INFO:root:Epoch no. 5 of 500 (samples: 141442)
INFO:root:Loss: 0.8332 +/- 0.0052
INFO:root:Epoch no. 6 of 500 (samples: 141442)
INFO:root:Loss: 0.8018 +/- 0.0043
INFO:root:Epoch no. 7 of 500 (samples: 141442)
INFO:root:Loss: 0.7784 +/- 0.0043
INFO:root:Epoch no. 8 of 500 (samples: 141442)
INFO:root:Loss: 0.7631 +/- 0.006
INFO:root:Epoch no. 9 of 500 (samples: 141442)
INFO:root:Loss: 0.7473 +/- 0.0044
INFO:root:Epoch no. 10 of 500 (samples: 141442)
INFO:root:Loss: 0.7317 +/- 0.0054
INFO:root:Epoch no. 11 of 500 (samples: 141442)
INFO:root:Loss: 0.7216 +/- 0.004
INFO:root:Epoch no. 12 of 500 (samples: 141442)
INFO:root:Loss: 0.7105 +/- 0.0056
INFO:root:Epoch no. 13 of 500 (samples: 141442)
INFO:root:Loss: 0.6996 +/- 0.0033
INFO:root:Epoch no. 14 of 500 (samples: 141442)
INFO:root:Loss: 0.691 +/- 0.0056
INFO:root:Epoch no. 15 of 500 (samples: 141442)
INFO:root:Loss: 0.6823 +/- 0.0074
INFO:root:Epoch no. 16 of 500 (samples: 141442)
INFO:root:Loss: 0.6735 +/- 0.0075
INFO:root:Epoch no. 17 of 500 (samples: 141442)
INFO:root:Loss: 0.6633 +/- 0.0063
INFO:root:Epoch no. 18 of 500 (samples: 141442)
INFO:root:Loss: 0.6555 +/- 0.0044
INFO:root:Epoch no. 19 of 500 (samples: 141442)
INFO:root:Loss: 0.6498 +/- 0.0039
INFO:root:Epoch no. 20 of 500 (samples: 141442)
INFO:root:Loss: 0.6399 +/- 0.0061
INFO:root:Epoch no. 21 of 500 (samples: 141442)
INFO:root:Loss: 0.6333 +/- 0.0061
INFO:root:Epoch no. 22 of 500 (samples: 141442)
INFO:root:Loss: 0.6251 +/- 0.0056
INFO:root:Epoch no. 23 of 500 (samples: 141442)
INFO:root:Loss: 0.6131 +/- 0.0043
INFO:root:Epoch no. 24 of 500 (samples: 141442)
INFO:root:Loss: 0.6049 +/- 0.0044
INFO:root:Epoch no. 25 of 500 (samples: 141442)
INFO:root:Loss: 0.5989 +/- 0.0052
INFO:root:Epoch no. 26 of 500 (samples: 141442)
INFO:root:Loss: 0.5927 +/- 0.0046
INFO:root:Epoch no. 27 of 500 (samples: 141442)
INFO:root:Loss: 0.585 +/- 0.005
INFO:root:Epoch no. 28 of 500 (samples: 141442)
INFO:root:Loss: 0.5786 +/- 0.0051
INFO:root:Epoch no. 29 of 500 (samples: 141442)
INFO:root:Loss: 0.5705 +/- 0.0051
INFO:root:Epoch no. 30 of 500 (samples: 141442)
INFO:root:Loss: 0.5624 +/- 0.005
INFO:root:Epoch no. 31 of 500 (samples: 141442)
INFO:root:Loss: 0.5543 +/- 0.0046
INFO:root:Epoch no. 32 of 500 (samples: 141442)
INFO:root:Loss: 0.5492 +/- 0.0039
INFO:root:Epoch no. 33 of 500 (samples: 141442)
INFO:root:Loss: 0.5391 +/- 0.0037
INFO:root:Epoch no. 34 of 500 (samples: 141442)
INFO:root:Loss: 0.5314 +/- 0.0043
INFO:root:Epoch no. 35 of 500 (samples: 141442)
INFO:root:Loss: 0.5294 +/- 0.0038
INFO:root:Epoch no. 36 of 500 (samples: 141442)
INFO:root:Loss: 0.5182 +/- 0.0042
INFO:root:Epoch no. 37 of 500 (samples: 141442)
INFO:root:Loss: 0.5123 +/- 0.0059
INFO:root:Epoch no. 38 of 500 (samples: 141442)
INFO:root:Loss: 0.5052 +/- 0.0079
INFO:root:Epoch no. 39 of 500 (samples: 141442)
INFO:root:Loss: 0.5005 +/- 0.0048
INFO:root:Epoch no. 40 of 500 (samples: 141442)
INFO:root:Loss: 0.4924 +/- 0.0051
INFO:root:Epoch no. 41 of 500 (samples: 141442)
INFO:root:Loss: 0.4872 +/- 0.0047
INFO:root:Epoch no. 42 of 500 (samples: 141442)
INFO:root:Loss: 0.48 +/- 0.0027
INFO:root:Epoch no. 43 of 500 (samples: 141442)
INFO:root:Loss: 0.4737 +/- 0.0062
INFO:root:Epoch no. 44 of 500 (samples: 141442)
INFO:root:Loss: 0.4689 +/- 0.005
INFO:root:Epoch no. 45 of 500 (samples: 141442)
INFO:root:Loss: 0.4616 +/- 0.0046
INFO:root:Epoch no. 46 of 500 (samples: 141442)
INFO:root:Loss: 0.4577 +/- 0.0035
INFO:root:Epoch no. 47 of 500 (samples: 141442)
INFO:root:Loss: 0.4494 +/- 0.005
INFO:root:Epoch no. 48 of 500 (samples: 141442)
INFO:root:Loss: 0.445 +/- 0.0035
INFO:root:Epoch no. 49 of 500 (samples: 141442)
INFO:root:Loss: 0.4387 +/- 0.0039
INFO:root:Epoch no. 50 of 500 (samples: 141442)
INFO:root:Loss: 0.4311 +/- 0.0037
INFO:root:Epoch no. 51 of 500 (samples: 141442)
INFO:root:Loss: 0.4276 +/- 0.0032
INFO:root:Epoch no. 52 of 500 (samples: 141442)
INFO:root:Loss: 0.4202 +/- 0.0024
INFO:root:Epoch no. 53 of 500 (samples: 141442)
INFO:root:Loss: 0.4174 +/- 0.0035
INFO:root:Epoch no. 54 of 500 (samples: 141442)
INFO:root:Loss: 0.411 +/- 0.0047
INFO:root:Epoch no. 55 of 500 (samples: 141442)
INFO:root:Loss: 0.4073 +/- 0.0047
INFO:root:Epoch no. 56 of 500 (samples: 141442)
INFO:root:Loss: 0.4005 +/- 0.0029
INFO:root:Epoch no. 57 of 500 (samples: 141442)
INFO:root:Loss: 0.3954 +/- 0.0049
INFO:root:Epoch no. 58 of 500 (samples: 141442)
INFO:root:Loss: 0.3913 +/- 0.0055
INFO:root:Epoch no. 59 of 500 (samples: 141442)
INFO:root:Loss: 0.3855 +/- 0.0038
INFO:root:Epoch no. 60 of 500 (samples: 141442)
INFO:root:Loss: 0.3829 +/- 0.003
INFO:root:Epoch no. 61 of 500 (samples: 141442)
INFO:root:Loss: 0.377 +/- 0.0051
INFO:root:Epoch no. 62 of 500 (samples: 141442)
INFO:root:Loss: 0.3719 +/- 0.005
INFO:root:Epoch no. 63 of 500 (samples: 141442)
INFO:root:Loss: 0.3666 +/- 0.0034
INFO:root:Epoch no. 64 of 500 (samples: 141442)
INFO:root:Loss: 0.3615 +/- 0.0026
INFO:root:Epoch no. 65 of 500 (samples: 141442)
INFO:root:Loss: 0.3583 +/- 0.0027
INFO:root:Epoch no. 66 of 500 (samples: 141442)
INFO:root:Loss: 0.3509 +/- 0.0033
INFO:root:Epoch no. 67 of 500 (samples: 141442)
INFO:root:Loss: 0.3491 +/- 0.0039
INFO:root:Epoch no. 68 of 500 (samples: 141442)
INFO:root:Loss: 0.345 +/- 0.0043
INFO:root:Epoch no. 69 of 500 (samples: 141442)
INFO:root:Loss: 0.3407 +/- 0.002
INFO:root:Epoch no. 70 of 500 (samples: 141442)
INFO:root:Loss: 0.334 +/- 0.0041
INFO:root:Epoch no. 71 of 500 (samples: 141442)
INFO:root:Loss: 0.3318 +/- 0.0039
INFO:root:Epoch no. 72 of 500 (samples: 141442)
INFO:root:Loss: 0.3272 +/- 0.0043
INFO:root:Epoch no. 73 of 500 (samples: 141442)
INFO:root:Loss: 0.3239 +/- 0.002
INFO:root:Epoch no. 74 of 500 (samples: 141442)
INFO:root:Loss: 0.3187 +/- 0.0029
INFO:root:Epoch no. 75 of 500 (samples: 141442)
INFO:root:Loss: 0.3142 +/- 0.0033
INFO:root:Epoch no. 76 of 500 (samples: 141442)
INFO:root:Loss: 0.3116 +/- 0.0044
INFO:root:Epoch no. 77 of 500 (samples: 141442)
INFO:root:Loss: 0.3048 +/- 0.0033
INFO:root:Epoch no. 78 of 500 (samples: 141442)
INFO:root:Loss: 0.3022 +/- 0.0042
INFO:root:Epoch no. 79 of 500 (samples: 141442)
INFO:root:Loss: 0.2999 +/- 0.0039
INFO:root:Epoch no. 80 of 500 (samples: 141442)
INFO:root:Loss: 0.2966 +/- 0.0027
INFO:root:Epoch no. 81 of 500 (samples: 141442)
INFO:root:Loss: 0.2923 +/- 0.0026
INFO:root:Epoch no. 82 of 500 (samples: 141442)
INFO:root:Loss: 0.2894 +/- 0.0024
INFO:root:Epoch no. 83 of 500 (samples: 141442)
INFO:root:Loss: 0.2829 +/- 0.0035
INFO:root:Epoch no. 84 of 500 (samples: 141442)
INFO:root:Loss: 0.2794 +/- 0.0041
INFO:root:Epoch no. 85 of 500 (samples: 141442)
INFO:root:Loss: 0.277 +/- 0.0032
INFO:root:Epoch no. 86 of 500 (samples: 141442)
INFO:root:Loss: 0.2717 +/- 0.0022
INFO:root:Epoch no. 87 of 500 (samples: 141442)
INFO:root:Loss: 0.269 +/- 0.0034
INFO:root:Epoch no. 88 of 500 (samples: 141442)
INFO:root:Loss: 0.2652 +/- 0.0036
INFO:root:Epoch no. 89 of 500 (samples: 141442)
INFO:root:Loss: 0.2628 +/- 0.0025
INFO:root:Epoch no. 90 of 500 (samples: 141442)
INFO:root:Loss: 0.2577 +/- 0.0032
INFO:root:Epoch no. 91 of 500 (samples: 141442)
INFO:root:Loss: 0.2555 +/- 0.0031
INFO:root:Epoch no. 92 of 500 (samples: 141442)
INFO:root:Loss: 0.2515 +/- 0.003
INFO:root:Epoch no. 93 of 500 (samples: 141442)
INFO:root:Loss: 0.2466 +/- 0.0027
INFO:root:Epoch no. 94 of 500 (samples: 141442)
INFO:root:Loss: 0.2451 +/- 0.0045
INFO:root:Epoch no. 95 of 500 (samples: 141442)
INFO:root:Loss: 0.2412 +/- 0.0012
INFO:root:Epoch no. 96 of 500 (samples: 141442)
INFO:root:Loss: 0.2381 +/- 0.0035
INFO:root:Epoch no. 97 of 500 (samples: 141442)
INFO:root:Loss: 0.2359 +/- 0.0023
INFO:root:Epoch no. 98 of 500 (samples: 141442)
INFO:root:Loss: 0.2327 +/- 0.0038
INFO:root:Epoch no. 99 of 500 (samples: 141442)
INFO:root:Loss: 0.2294 +/- 0.0028
INFO:root:Epoch no. 100 of 500 (samples: 141442)
INFO:root:Loss: 0.2272 +/- 0.0022
INFO:root:Epoch no. 101 of 500 (samples: 141442)
INFO:root:Loss: 0.2229 +/- 0.0022
INFO:root:Epoch no. 102 of 500 (samples: 141442)
INFO:root:Loss: 0.2204 +/- 0.0022
INFO:root:Epoch no. 103 of 500 (samples: 141442)
INFO:root:Loss: 0.2176 +/- 0.0021
INFO:root:Epoch no. 104 of 500 (samples: 141442)
INFO:root:Loss: 0.215 +/- 0.0027
INFO:root:Epoch no. 105 of 500 (samples: 141442)
INFO:root:Loss: 0.2106 +/- 0.0019
INFO:root:Epoch no. 106 of 500 (samples: 141442)
INFO:root:Loss: 0.2093 +/- 0.0033
INFO:root:Epoch no. 107 of 500 (samples: 141442)
INFO:root:Loss: 0.2055 +/- 0.0023
INFO:root:Epoch no. 108 of 500 (samples: 141442)
INFO:root:Loss: 0.2022 +/- 0.0022
INFO:root:Epoch no. 109 of 500 (samples: 141442)
INFO:root:Loss: 0.1997 +/- 0.002
INFO:root:Epoch no. 110 of 500 (samples: 141442)
INFO:root:Loss: 0.1973 +/- 0.0033
INFO:root:Epoch no. 111 of 500 (samples: 141442)
INFO:root:Loss: 0.1951 +/- 0.003
INFO:root:Epoch no. 112 of 500 (samples: 141442)
INFO:root:Loss: 0.1921 +/- 0.002
INFO:root:Epoch no. 113 of 500 (samples: 141442)
INFO:root:Loss: 0.1896 +/- 0.0022
INFO:root:Epoch no. 114 of 500 (samples: 141442)
INFO:root:Loss: 0.1876 +/- 0.0025
INFO:root:Epoch no. 115 of 500 (samples: 141442)
INFO:root:Loss: 0.1837 +/- 0.0027
INFO:root:Epoch no. 116 of 500 (samples: 141442)
INFO:root:Loss: 0.1826 +/- 0.0014
INFO:root:Epoch no. 117 of 500 (samples: 141442)
INFO:root:Loss: 0.1791 +/- 0.0017
INFO:root:Epoch no. 118 of 500 (samples: 141442)
INFO:root:Loss: 0.1768 +/- 0.0025
INFO:root:Epoch no. 119 of 500 (samples: 141442)
INFO:root:Loss: 0.1746 +/- 0.0031
INFO:root:Epoch no. 120 of 500 (samples: 141442)
INFO:root:Loss: 0.1722 +/- 0.0027
INFO:root:Epoch no. 121 of 500 (samples: 141442)
INFO:root:Loss: 0.1716 +/- 0.002
INFO:root:Epoch no. 122 of 500 (samples: 141442)
INFO:root:Loss: 0.1678 +/- 0.002
INFO:root:Epoch no. 123 of 500 (samples: 141442)
INFO:root:Loss: 0.1668 +/- 0.0021
INFO:root:Epoch no. 124 of 500 (samples: 141442)
INFO:root:Loss: 0.1646 +/- 0.002
INFO:root:Epoch no. 125 of 500 (samples: 141442)
INFO:root:Loss: 0.1625 +/- 0.0016
INFO:root:Epoch no. 126 of 500 (samples: 141442)
INFO:root:Loss: 0.1601 +/- 0.0031
INFO:root:Epoch no. 127 of 500 (samples: 141442)
INFO:root:Loss: 0.159 +/- 0.0022
INFO:root:Epoch no. 128 of 500 (samples: 141442)
INFO:root:Loss: 0.1575 +/- 0.0018
INFO:root:Epoch no. 129 of 500 (samples: 141442)
INFO:root:Loss: 0.1536 +/- 0.0021
INFO:root:Epoch no. 130 of 500 (samples: 141442)
INFO:root:Loss: 0.1523 +/- 0.0018
INFO:root:Epoch no. 131 of 500 (samples: 141442)
INFO:root:Loss: 0.1491 +/- 0.0019
INFO:root:Epoch no. 132 of 500 (samples: 141442)
INFO:root:Loss: 0.1479 +/- 0.001
INFO:root:Epoch no. 133 of 500 (samples: 141442)
INFO:root:Loss: 0.1453 +/- 0.0019
INFO:root:Epoch no. 134 of 500 (samples: 141442)
INFO:root:Loss: 0.1449 +/- 0.0011
INFO:root:Epoch no. 135 of 500 (samples: 141442)
INFO:root:Loss: 0.1424 +/- 0.0026
INFO:root:Epoch no. 136 of 500 (samples: 141442)
INFO:root:Loss: 0.1403 +/- 0.002
INFO:root:Epoch no. 137 of 500 (samples: 141442)
INFO:root:Loss: 0.1383 +/- 0.0014
INFO:root:Epoch no. 138 of 500 (samples: 141442)
INFO:root:Loss: 0.137 +/- 0.0025
INFO:root:Epoch no. 139 of 500 (samples: 141442)
INFO:root:Loss: 0.1347 +/- 0.0019
INFO:root:Epoch no. 140 of 500 (samples: 141442)
INFO:root:Loss: 0.1327 +/- 0.0021
INFO:root:Epoch no. 141 of 500 (samples: 141442)
INFO:root:Loss: 0.1315 +/- 0.0018
INFO:root:Epoch no. 142 of 500 (samples: 141442)
Traceback (most recent call last):
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./bin/hyper-cli.py", line 324, in <module>
    main(sys.argv[1:])
  File "./bin/hyper-cli.py", line 293, in main
    visualize=is_visualize)
  File "/home/insight/.local/lib/python3.4/site-packages/hyper-0.0.1-py3.4.egg/hyper/learning/core.py", line 185, in pairwise_training
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/models.py", line 409, in fit
    sample_weight=sample_weight)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 1052, in fit
    callback_metrics=callback_metrics)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 790, in _fit_loop
    outs = f(ins_batch)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/backend/theano_backend.py", line 518, in __call__
    return self.function(*inputs)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 871, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/usr/local/lib/python3.4/dist-packages/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/usr/local/lib/python3.4/dist-packages/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: 
Apply node that caused the error: Elemwise{mul}(Reshape{3}.0, InplaceDimShuffle{0,x,x}.0, InplaceDimShuffle{0,x,1}.0)
Toposort index: 91
Inputs types: [TensorType(float32, (False, False, True)), TensorType(float32, (False, True, True)), TensorType(float32, (False, True, False))]
Inputs shapes: [(42435, 100, 1), (42435, 1, 1), (42435, 1, 100)]
Inputs strides: [(800, 4, 4), (4, 4, 4), (800, 400, 4)]
Inputs values: ['not shown', 'not shown', 'not shown']
Outputs clients: [[Reshape{2}(Elemwise{mul}.0, MakeVector{dtype='int64'}.0)]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
