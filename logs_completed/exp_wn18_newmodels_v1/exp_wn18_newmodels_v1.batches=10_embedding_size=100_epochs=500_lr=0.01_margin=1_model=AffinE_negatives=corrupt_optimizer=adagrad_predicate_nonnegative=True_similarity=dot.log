Using Theano backend.
INFO:root:Acquiring data/wn18/wordnet-mlj12-train.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-valid.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-test.txt ..
INFO:root:Samples: 141442, no. batches: 10 -> batch size: 14145
INFO:root:Epoch no. 1 of 500 (samples: 141442)
INFO:root:Loss: 0.6427 +/- 0.0171
INFO:root:Epoch no. 2 of 500 (samples: 141442)
INFO:root:Loss: 0.5295 +/- 0.0313
INFO:root:Epoch no. 3 of 500 (samples: 141442)
INFO:root:Loss: 0.4006 +/- 0.021
INFO:root:Epoch no. 4 of 500 (samples: 141442)
INFO:root:Loss: 0.319 +/- 0.0095
INFO:root:Epoch no. 5 of 500 (samples: 141442)
INFO:root:Loss: 0.2713 +/- 0.0058
INFO:root:Epoch no. 6 of 500 (samples: 141442)
INFO:root:Loss: 0.241 +/- 0.0036
INFO:root:Epoch no. 7 of 500 (samples: 141442)
INFO:root:Loss: 0.2193 +/- 0.0026
INFO:root:Epoch no. 8 of 500 (samples: 141442)
INFO:root:Loss: 0.2014 +/- 0.0027
INFO:root:Epoch no. 9 of 500 (samples: 141442)
INFO:root:Loss: 0.1877 +/- 0.0022
INFO:root:Epoch no. 10 of 500 (samples: 141442)
INFO:root:Loss: 0.1751 +/- 0.0028
INFO:root:Epoch no. 11 of 500 (samples: 141442)
INFO:root:Loss: 0.1669 +/- 0.0018
INFO:root:Epoch no. 12 of 500 (samples: 141442)
INFO:root:Loss: 0.1575 +/- 0.0026
INFO:root:Epoch no. 13 of 500 (samples: 141442)
INFO:root:Loss: 0.1504 +/- 0.0014
INFO:root:Epoch no. 14 of 500 (samples: 141442)
INFO:root:Loss: 0.1449 +/- 0.002
INFO:root:Epoch no. 15 of 500 (samples: 141442)
INFO:root:Loss: 0.1389 +/- 0.0015
INFO:root:Epoch no. 16 of 500 (samples: 141442)
INFO:root:Loss: 0.1328 +/- 0.0018
INFO:root:Epoch no. 17 of 500 (samples: 141442)
INFO:root:Loss: 0.1279 +/- 0.0019
INFO:root:Epoch no. 18 of 500 (samples: 141442)
INFO:root:Loss: 0.1233 +/- 0.0017
INFO:root:Epoch no. 19 of 500 (samples: 141442)
INFO:root:Loss: 0.1204 +/- 0.0018
INFO:root:Epoch no. 20 of 500 (samples: 141442)
INFO:root:Loss: 0.1167 +/- 0.0015
INFO:root:Epoch no. 21 of 500 (samples: 141442)
INFO:root:Loss: 0.1132 +/- 0.0012
INFO:root:Epoch no. 22 of 500 (samples: 141442)
INFO:root:Loss: 0.1104 +/- 0.0017
INFO:root:Epoch no. 23 of 500 (samples: 141442)
INFO:root:Loss: 0.1067 +/- 0.0019
INFO:root:Epoch no. 24 of 500 (samples: 141442)
INFO:root:Loss: 0.1045 +/- 0.0014
INFO:root:Epoch no. 25 of 500 (samples: 141442)
INFO:root:Loss: 0.1011 +/- 0.0012
INFO:root:Epoch no. 26 of 500 (samples: 141442)
INFO:root:Loss: 0.0983 +/- 0.002
INFO:root:Epoch no. 27 of 500 (samples: 141442)
INFO:root:Loss: 0.0965 +/- 0.0016
INFO:root:Epoch no. 28 of 500 (samples: 141442)
INFO:root:Loss: 0.0953 +/- 0.0017
INFO:root:Epoch no. 29 of 500 (samples: 141442)
INFO:root:Loss: 0.0927 +/- 0.0015
INFO:root:Epoch no. 30 of 500 (samples: 141442)
INFO:root:Loss: 0.0907 +/- 0.0015
INFO:root:Epoch no. 31 of 500 (samples: 141442)
INFO:root:Loss: 0.0885 +/- 0.0012
INFO:root:Epoch no. 32 of 500 (samples: 141442)
INFO:root:Loss: 0.0863 +/- 0.0014
INFO:root:Epoch no. 33 of 500 (samples: 141442)
INFO:root:Loss: 0.0851 +/- 0.0014
INFO:root:Epoch no. 34 of 500 (samples: 141442)
INFO:root:Loss: 0.0837 +/- 0.0015
INFO:root:Epoch no. 35 of 500 (samples: 141442)
INFO:root:Loss: 0.0818 +/- 0.001
INFO:root:Epoch no. 36 of 500 (samples: 141442)
INFO:root:Loss: 0.0804 +/- 0.0011
INFO:root:Epoch no. 37 of 500 (samples: 141442)
INFO:root:Loss: 0.0794 +/- 0.0018
INFO:root:Epoch no. 38 of 500 (samples: 141442)
INFO:root:Loss: 0.0775 +/- 0.0017
INFO:root:Epoch no. 39 of 500 (samples: 141442)
INFO:root:Loss: 0.0759 +/- 0.0013
INFO:root:Epoch no. 40 of 500 (samples: 141442)
INFO:root:Loss: 0.0742 +/- 0.0012
INFO:root:Epoch no. 41 of 500 (samples: 141442)
INFO:root:Loss: 0.0729 +/- 0.0017
INFO:root:Epoch no. 42 of 500 (samples: 141442)
INFO:root:Loss: 0.0725 +/- 0.0016
INFO:root:Epoch no. 43 of 500 (samples: 141442)
INFO:root:Loss: 0.071 +/- 0.0012
INFO:root:Epoch no. 44 of 500 (samples: 141442)
INFO:root:Loss: 0.0704 +/- 0.0007
INFO:root:Epoch no. 45 of 500 (samples: 141442)
INFO:root:Loss: 0.0689 +/- 0.001
INFO:root:Epoch no. 46 of 500 (samples: 141442)
INFO:root:Loss: 0.0684 +/- 0.0011
INFO:root:Epoch no. 47 of 500 (samples: 141442)
INFO:root:Loss: 0.0669 +/- 0.0011
INFO:root:Epoch no. 48 of 500 (samples: 141442)
INFO:root:Loss: 0.0663 +/- 0.0012
INFO:root:Epoch no. 49 of 500 (samples: 141442)
INFO:root:Loss: 0.0647 +/- 0.0008
INFO:root:Epoch no. 50 of 500 (samples: 141442)
INFO:root:Loss: 0.0645 +/- 0.0011
INFO:root:Epoch no. 51 of 500 (samples: 141442)
INFO:root:Loss: 0.0641 +/- 0.0009
INFO:root:Epoch no. 52 of 500 (samples: 141442)
INFO:root:Loss: 0.0626 +/- 0.0019
INFO:root:Epoch no. 53 of 500 (samples: 141442)
INFO:root:Loss: 0.0617 +/- 0.0011
INFO:root:Epoch no. 54 of 500 (samples: 141442)
INFO:root:Loss: 0.0606 +/- 0.0006
INFO:root:Epoch no. 55 of 500 (samples: 141442)
INFO:root:Loss: 0.0603 +/- 0.0012
INFO:root:Epoch no. 56 of 500 (samples: 141442)
INFO:root:Loss: 0.0591 +/- 0.0004
INFO:root:Epoch no. 57 of 500 (samples: 141442)
INFO:root:Loss: 0.0587 +/- 0.0011
INFO:root:Epoch no. 58 of 500 (samples: 141442)
INFO:root:Loss: 0.0583 +/- 0.0009
INFO:root:Epoch no. 59 of 500 (samples: 141442)
INFO:root:Loss: 0.0572 +/- 0.001
INFO:root:Epoch no. 60 of 500 (samples: 141442)
INFO:root:Loss: 0.0565 +/- 0.0012
INFO:root:Epoch no. 61 of 500 (samples: 141442)
INFO:root:Loss: 0.0558 +/- 0.0011
INFO:root:Epoch no. 62 of 500 (samples: 141442)
INFO:root:Loss: 0.0556 +/- 0.0013
INFO:root:Epoch no. 63 of 500 (samples: 141442)
INFO:root:Loss: 0.0545 +/- 0.0012
INFO:root:Epoch no. 64 of 500 (samples: 141442)
INFO:root:Loss: 0.0537 +/- 0.0006
INFO:root:Epoch no. 65 of 500 (samples: 141442)
INFO:root:Loss: 0.0533 +/- 0.0006
INFO:root:Epoch no. 66 of 500 (samples: 141442)
INFO:root:Loss: 0.0529 +/- 0.0011
INFO:root:Epoch no. 67 of 500 (samples: 141442)
INFO:root:Loss: 0.0527 +/- 0.001
INFO:root:Epoch no. 68 of 500 (samples: 141442)
INFO:root:Loss: 0.0517 +/- 0.0008
INFO:root:Epoch no. 69 of 500 (samples: 141442)
INFO:root:Loss: 0.0506 +/- 0.001
INFO:root:Epoch no. 70 of 500 (samples: 141442)
INFO:root:Loss: 0.0507 +/- 0.0011
INFO:root:Epoch no. 71 of 500 (samples: 141442)
INFO:root:Loss: 0.0504 +/- 0.0015
INFO:root:Epoch no. 72 of 500 (samples: 141442)
INFO:root:Loss: 0.0499 +/- 0.0011
INFO:root:Epoch no. 73 of 500 (samples: 141442)
INFO:root:Loss: 0.0494 +/- 0.0008
INFO:root:Epoch no. 74 of 500 (samples: 141442)
INFO:root:Loss: 0.0482 +/- 0.001
INFO:root:Epoch no. 75 of 500 (samples: 141442)
INFO:root:Loss: 0.0482 +/- 0.0015
INFO:root:Epoch no. 76 of 500 (samples: 141442)
INFO:root:Loss: 0.0475 +/- 0.0006
INFO:root:Epoch no. 77 of 500 (samples: 141442)
INFO:root:Loss: 0.0473 +/- 0.0011
INFO:root:Epoch no. 78 of 500 (samples: 141442)
INFO:root:Loss: 0.0463 +/- 0.0008
INFO:root:Epoch no. 79 of 500 (samples: 141442)
INFO:root:Loss: 0.0469 +/- 0.0009
INFO:root:Epoch no. 80 of 500 (samples: 141442)
INFO:root:Loss: 0.0462 +/- 0.0015
INFO:root:Epoch no. 81 of 500 (samples: 141442)
INFO:root:Loss: 0.0456 +/- 0.0007
INFO:root:Epoch no. 82 of 500 (samples: 141442)
INFO:root:Loss: 0.0452 +/- 0.0007
INFO:root:Epoch no. 83 of 500 (samples: 141442)
INFO:root:Loss: 0.0449 +/- 0.0008
INFO:root:Epoch no. 84 of 500 (samples: 141442)
INFO:root:Loss: 0.0442 +/- 0.0008
INFO:root:Epoch no. 85 of 500 (samples: 141442)
INFO:root:Loss: 0.0439 +/- 0.0012
INFO:root:Epoch no. 86 of 500 (samples: 141442)
INFO:root:Loss: 0.0433 +/- 0.0009
INFO:root:Epoch no. 87 of 500 (samples: 141442)
INFO:root:Loss: 0.0434 +/- 0.0007
INFO:root:Epoch no. 88 of 500 (samples: 141442)
INFO:root:Loss: 0.0428 +/- 0.0007
INFO:root:Epoch no. 89 of 500 (samples: 141442)
INFO:root:Loss: 0.0425 +/- 0.0011
INFO:root:Epoch no. 90 of 500 (samples: 141442)
INFO:root:Loss: 0.0423 +/- 0.0008
INFO:root:Epoch no. 91 of 500 (samples: 141442)
INFO:root:Loss: 0.0417 +/- 0.0008
INFO:root:Epoch no. 92 of 500 (samples: 141442)
INFO:root:Loss: 0.0417 +/- 0.0011
INFO:root:Epoch no. 93 of 500 (samples: 141442)
INFO:root:Loss: 0.0412 +/- 0.0004
INFO:root:Epoch no. 94 of 500 (samples: 141442)
INFO:root:Loss: 0.0414 +/- 0.0007
INFO:root:Epoch no. 95 of 500 (samples: 141442)
INFO:root:Loss: 0.0402 +/- 0.001
INFO:root:Epoch no. 96 of 500 (samples: 141442)
INFO:root:Loss: 0.0404 +/- 0.001
INFO:root:Epoch no. 97 of 500 (samples: 141442)
INFO:root:Loss: 0.0395 +/- 0.001
INFO:root:Epoch no. 98 of 500 (samples: 141442)
INFO:root:Loss: 0.0398 +/- 0.0015
INFO:root:Epoch no. 99 of 500 (samples: 141442)
INFO:root:Loss: 0.0397 +/- 0.0011
INFO:root:Epoch no. 100 of 500 (samples: 141442)
INFO:root:Loss: 0.039 +/- 0.0008
INFO:root:Epoch no. 101 of 500 (samples: 141442)
INFO:root:Loss: 0.0386 +/- 0.001
INFO:root:Epoch no. 102 of 500 (samples: 141442)
INFO:root:Loss: 0.0383 +/- 0.0009
INFO:root:Epoch no. 103 of 500 (samples: 141442)
INFO:root:Loss: 0.0381 +/- 0.0009
INFO:root:Epoch no. 104 of 500 (samples: 141442)
INFO:root:Loss: 0.0376 +/- 0.0011
INFO:root:Epoch no. 105 of 500 (samples: 141442)
INFO:root:Loss: 0.038 +/- 0.001
INFO:root:Epoch no. 106 of 500 (samples: 141442)
INFO:root:Loss: 0.0373 +/- 0.0005
INFO:root:Epoch no. 107 of 500 (samples: 141442)
INFO:root:Loss: 0.0371 +/- 0.001
INFO:root:Epoch no. 108 of 500 (samples: 141442)
INFO:root:Loss: 0.0363 +/- 0.0007
INFO:root:Epoch no. 109 of 500 (samples: 141442)
INFO:root:Loss: 0.0365 +/- 0.001
INFO:root:Epoch no. 110 of 500 (samples: 141442)
INFO:root:Loss: 0.0359 +/- 0.0007
INFO:root:Epoch no. 111 of 500 (samples: 141442)
INFO:root:Loss: 0.0356 +/- 0.0005
INFO:root:Epoch no. 112 of 500 (samples: 141442)
INFO:root:Loss: 0.036 +/- 0.0006
INFO:root:Epoch no. 113 of 500 (samples: 141442)
INFO:root:Loss: 0.0358 +/- 0.0009
INFO:root:Epoch no. 114 of 500 (samples: 141442)
INFO:root:Loss: 0.0357 +/- 0.0007
INFO:root:Epoch no. 115 of 500 (samples: 141442)
INFO:root:Loss: 0.0352 +/- 0.0008
INFO:root:Epoch no. 116 of 500 (samples: 141442)
INFO:root:Loss: 0.0346 +/- 0.0009
INFO:root:Epoch no. 117 of 500 (samples: 141442)
INFO:root:Loss: 0.0347 +/- 0.0005
INFO:root:Epoch no. 118 of 500 (samples: 141442)
INFO:root:Loss: 0.0343 +/- 0.0009
INFO:root:Epoch no. 119 of 500 (samples: 141442)
INFO:root:Loss: 0.0343 +/- 0.0012
INFO:root:Epoch no. 120 of 500 (samples: 141442)
INFO:root:Loss: 0.0337 +/- 0.0008
INFO:root:Epoch no. 121 of 500 (samples: 141442)
INFO:root:Loss: 0.0341 +/- 0.001
INFO:root:Epoch no. 122 of 500 (samples: 141442)
INFO:root:Loss: 0.0335 +/- 0.0006
INFO:root:Epoch no. 123 of 500 (samples: 141442)
INFO:root:Loss: 0.0337 +/- 0.0006
INFO:root:Epoch no. 124 of 500 (samples: 141442)
INFO:root:Loss: 0.0331 +/- 0.0005
INFO:root:Epoch no. 125 of 500 (samples: 141442)
INFO:root:Loss: 0.0329 +/- 0.0006
INFO:root:Epoch no. 126 of 500 (samples: 141442)
INFO:root:Loss: 0.033 +/- 0.0004
INFO:root:Epoch no. 127 of 500 (samples: 141442)
INFO:root:Loss: 0.0329 +/- 0.0008
INFO:root:Epoch no. 128 of 500 (samples: 141442)
INFO:root:Loss: 0.0322 +/- 0.001
INFO:root:Epoch no. 129 of 500 (samples: 141442)
INFO:root:Loss: 0.0324 +/- 0.0011
INFO:root:Epoch no. 130 of 500 (samples: 141442)
INFO:root:Loss: 0.0318 +/- 0.0009
INFO:root:Epoch no. 131 of 500 (samples: 141442)
INFO:root:Loss: 0.0318 +/- 0.0006
INFO:root:Epoch no. 132 of 500 (samples: 141442)
INFO:root:Loss: 0.0315 +/- 0.0009
INFO:root:Epoch no. 133 of 500 (samples: 141442)
INFO:root:Loss: 0.031 +/- 0.0008
INFO:root:Epoch no. 134 of 500 (samples: 141442)
INFO:root:Loss: 0.0311 +/- 0.0007
INFO:root:Epoch no. 135 of 500 (samples: 141442)
INFO:root:Loss: 0.0312 +/- 0.0009
INFO:root:Epoch no. 136 of 500 (samples: 141442)
INFO:root:Loss: 0.0311 +/- 0.0008
INFO:root:Epoch no. 137 of 500 (samples: 141442)
INFO:root:Loss: 0.0302 +/- 0.0007
INFO:root:Epoch no. 138 of 500 (samples: 141442)
INFO:root:Loss: 0.0305 +/- 0.001
INFO:root:Epoch no. 139 of 500 (samples: 141442)
INFO:root:Loss: 0.0305 +/- 0.0009
INFO:root:Epoch no. 140 of 500 (samples: 141442)
INFO:root:Loss: 0.0298 +/- 0.0006
INFO:root:Epoch no. 141 of 500 (samples: 141442)
INFO:root:Loss: 0.0299 +/- 0.0007
INFO:root:Epoch no. 142 of 500 (samples: 141442)
INFO:root:Loss: 0.0299 +/- 0.0007
INFO:root:Epoch no. 143 of 500 (samples: 141442)
INFO:root:Loss: 0.0297 +/- 0.0006
INFO:root:Epoch no. 144 of 500 (samples: 141442)
INFO:root:Loss: 0.0294 +/- 0.0007
INFO:root:Epoch no. 145 of 500 (samples: 141442)
INFO:root:Loss: 0.0292 +/- 0.0006
INFO:root:Epoch no. 146 of 500 (samples: 141442)
INFO:root:Loss: 0.029 +/- 0.0007
INFO:root:Epoch no. 147 of 500 (samples: 141442)
INFO:root:Loss: 0.0289 +/- 0.0008
INFO:root:Epoch no. 148 of 500 (samples: 141442)
INFO:root:Loss: 0.0289 +/- 0.0006
INFO:root:Epoch no. 149 of 500 (samples: 141442)
INFO:root:Loss: 0.0286 +/- 0.0005
INFO:root:Epoch no. 150 of 500 (samples: 141442)
INFO:root:Loss: 0.0289 +/- 0.0007
INFO:root:Epoch no. 151 of 500 (samples: 141442)
INFO:root:Loss: 0.0283 +/- 0.0005
INFO:root:Epoch no. 152 of 500 (samples: 141442)
INFO:root:Loss: 0.0276 +/- 0.0007
INFO:root:Epoch no. 153 of 500 (samples: 141442)
INFO:root:Loss: 0.0283 +/- 0.0005
INFO:root:Epoch no. 154 of 500 (samples: 141442)
INFO:root:Loss: 0.028 +/- 0.0005
INFO:root:Epoch no. 155 of 500 (samples: 141442)
INFO:root:Loss: 0.0277 +/- 0.0006
INFO:root:Epoch no. 156 of 500 (samples: 141442)
INFO:root:Loss: 0.0276 +/- 0.0008
INFO:root:Epoch no. 157 of 500 (samples: 141442)
INFO:root:Loss: 0.0278 +/- 0.0006
INFO:root:Epoch no. 158 of 500 (samples: 141442)
INFO:root:Loss: 0.0269 +/- 0.001
INFO:root:Epoch no. 159 of 500 (samples: 141442)
INFO:root:Loss: 0.0271 +/- 0.0007
INFO:root:Epoch no. 160 of 500 (samples: 141442)
INFO:root:Loss: 0.027 +/- 0.0004
INFO:root:Epoch no. 161 of 500 (samples: 141442)
INFO:root:Loss: 0.0269 +/- 0.0005
INFO:root:Epoch no. 162 of 500 (samples: 141442)
INFO:root:Loss: 0.0269 +/- 0.0004
INFO:root:Epoch no. 163 of 500 (samples: 141442)
INFO:root:Loss: 0.0262 +/- 0.0006
INFO:root:Epoch no. 164 of 500 (samples: 141442)
INFO:root:Loss: 0.0263 +/- 0.0007
INFO:root:Epoch no. 165 of 500 (samples: 141442)
INFO:root:Loss: 0.0262 +/- 0.0008
INFO:root:Epoch no. 166 of 500 (samples: 141442)
INFO:root:Loss: 0.0264 +/- 0.0005
INFO:root:Epoch no. 167 of 500 (samples: 141442)
INFO:root:Loss: 0.0262 +/- 0.0007
INFO:root:Epoch no. 168 of 500 (samples: 141442)
INFO:root:Loss: 0.0259 +/- 0.0005
INFO:root:Epoch no. 169 of 500 (samples: 141442)
INFO:root:Loss: 0.0259 +/- 0.0008
INFO:root:Epoch no. 170 of 500 (samples: 141442)
INFO:root:Loss: 0.0259 +/- 0.0006
INFO:root:Epoch no. 171 of 500 (samples: 141442)
INFO:root:Loss: 0.0257 +/- 0.0005
INFO:root:Epoch no. 172 of 500 (samples: 141442)
INFO:root:Loss: 0.026 +/- 0.0005
INFO:root:Epoch no. 173 of 500 (samples: 141442)
INFO:root:Loss: 0.0254 +/- 0.0007
INFO:root:Epoch no. 174 of 500 (samples: 141442)
INFO:root:Loss: 0.0248 +/- 0.0005
INFO:root:Epoch no. 175 of 500 (samples: 141442)
INFO:root:Loss: 0.0253 +/- 0.0006
INFO:root:Epoch no. 176 of 500 (samples: 141442)
INFO:root:Loss: 0.025 +/- 0.0008
INFO:root:Epoch no. 177 of 500 (samples: 141442)
INFO:root:Loss: 0.025 +/- 0.0005
INFO:root:Epoch no. 178 of 500 (samples: 141442)
INFO:root:Loss: 0.025 +/- 0.001
INFO:root:Epoch no. 179 of 500 (samples: 141442)
INFO:root:Loss: 0.025 +/- 0.0005
INFO:root:Epoch no. 180 of 500 (samples: 141442)
INFO:root:Loss: 0.0246 +/- 0.0006
INFO:root:Epoch no. 181 of 500 (samples: 141442)
INFO:root:Loss: 0.0244 +/- 0.0005
INFO:root:Epoch no. 182 of 500 (samples: 141442)
INFO:root:Loss: 0.024 +/- 0.0003
INFO:root:Epoch no. 183 of 500 (samples: 141442)
INFO:root:Loss: 0.0243 +/- 0.0006
INFO:root:Epoch no. 184 of 500 (samples: 141442)
INFO:root:Loss: 0.0241 +/- 0.0007
INFO:root:Epoch no. 185 of 500 (samples: 141442)
INFO:root:Loss: 0.0238 +/- 0.0005
INFO:root:Epoch no. 186 of 500 (samples: 141442)
INFO:root:Loss: 0.0241 +/- 0.0007
INFO:root:Epoch no. 187 of 500 (samples: 141442)
INFO:root:Loss: 0.0237 +/- 0.0006
INFO:root:Epoch no. 188 of 500 (samples: 141442)
INFO:root:Loss: 0.024 +/- 0.0007
INFO:root:Epoch no. 189 of 500 (samples: 141442)
INFO:root:Loss: 0.0236 +/- 0.0008
INFO:root:Epoch no. 190 of 500 (samples: 141442)
INFO:root:Loss: 0.0234 +/- 0.0006
INFO:root:Epoch no. 191 of 500 (samples: 141442)
INFO:root:Loss: 0.0238 +/- 0.0006
INFO:root:Epoch no. 192 of 500 (samples: 141442)
INFO:root:Loss: 0.023 +/- 0.0006
INFO:root:Epoch no. 193 of 500 (samples: 141442)
INFO:root:Loss: 0.0233 +/- 0.0007
INFO:root:Epoch no. 194 of 500 (samples: 141442)
INFO:root:Loss: 0.0232 +/- 0.0005
INFO:root:Epoch no. 195 of 500 (samples: 141442)
INFO:root:Loss: 0.0228 +/- 0.0005
INFO:root:Epoch no. 196 of 500 (samples: 141442)
INFO:root:Loss: 0.0229 +/- 0.0006
INFO:root:Epoch no. 197 of 500 (samples: 141442)
INFO:root:Loss: 0.0228 +/- 0.0005
INFO:root:Epoch no. 198 of 500 (samples: 141442)
INFO:root:Loss: 0.0228 +/- 0.0004
INFO:root:Epoch no. 199 of 500 (samples: 141442)
INFO:root:Loss: 0.0229 +/- 0.0005
INFO:root:Epoch no. 200 of 500 (samples: 141442)
INFO:root:Loss: 0.0228 +/- 0.0005
INFO:root:Epoch no. 201 of 500 (samples: 141442)
INFO:root:Loss: 0.0226 +/- 0.0006
INFO:root:Epoch no. 202 of 500 (samples: 141442)
INFO:root:Loss: 0.0221 +/- 0.0007
INFO:root:Epoch no. 203 of 500 (samples: 141442)
INFO:root:Loss: 0.0223 +/- 0.0004
INFO:root:Epoch no. 204 of 500 (samples: 141442)
INFO:root:Loss: 0.022 +/- 0.0003
INFO:root:Epoch no. 205 of 500 (samples: 141442)
INFO:root:Loss: 0.0218 +/- 0.0006
INFO:root:Epoch no. 206 of 500 (samples: 141442)
Traceback (most recent call last):
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: alloc failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./bin/hyper-cli.py", line 324, in <module>
    main(sys.argv[1:])
  File "./bin/hyper-cli.py", line 293, in main
    visualize=is_visualize)
  File "/home/insight/.local/lib/python3.4/site-packages/hyper-0.0.1-py3.4.egg/hyper/learning/core.py", line 185, in pairwise_training
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/models.py", line 409, in fit
    sample_weight=sample_weight)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 1052, in fit
    callback_metrics=callback_metrics)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 790, in _fit_loop
    outs = f(ins_batch)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/backend/theano_backend.py", line 518, in __call__
    return self.function(*inputs)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 871, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/usr/local/lib/python3.4/dist-packages/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/usr/local/lib/python3.4/dist-packages/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: alloc failed
Apply node that caused the error: Alloc(TensorConstant{(1, 1) of 0.0}, Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}.0, Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}[(0, 0)].0)
Toposort index: 51
Inputs types: [TensorType(float32, (True, True)), TensorType(int64, scalar), TensorType(int64, scalar)]
Inputs shapes: [(1, 1), (), ()]
Inputs strides: [(4, 4), (), ()]
Inputs values: [array([[ 0.]], dtype=float32), array(42435), array(10100)]
Outputs clients: [[IncSubtensor{InplaceInc;::, :int64:}(Alloc.0, Reshape{2}.0, ScalarFromTensor.0)]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
