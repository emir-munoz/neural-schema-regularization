Using Theano backend.
INFO:root:Acquiring data/wn18/wordnet-mlj12-train.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-valid.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-test.txt ..
INFO:root:Samples: 141442, no. batches: 10 -> batch size: 14145
INFO:root:Epoch no. 1 of 500 (samples: 141442)
INFO:root:Loss: 3.3326 +/- 0.0007
INFO:root:Epoch no. 2 of 500 (samples: 141442)
INFO:root:Loss: 3.3242 +/- 0.0014
INFO:root:Epoch no. 3 of 500 (samples: 141442)
INFO:root:Loss: 3.3133 +/- 0.0025
INFO:root:Epoch no. 4 of 500 (samples: 141442)
INFO:root:Loss: 3.2981 +/- 0.0037
INFO:root:Epoch no. 5 of 500 (samples: 141442)
INFO:root:Loss: 3.277 +/- 0.0052
INFO:root:Epoch no. 6 of 500 (samples: 141442)
INFO:root:Loss: 3.2493 +/- 0.0071
INFO:root:Epoch no. 7 of 500 (samples: 141442)
INFO:root:Loss: 3.2137 +/- 0.0089
INFO:root:Epoch no. 8 of 500 (samples: 141442)
INFO:root:Loss: 3.17 +/- 0.0111
INFO:root:Epoch no. 9 of 500 (samples: 141442)
INFO:root:Loss: 3.12 +/- 0.0118
INFO:root:Epoch no. 10 of 500 (samples: 141442)
INFO:root:Loss: 3.0647 +/- 0.0134
INFO:root:Epoch no. 11 of 500 (samples: 141442)
INFO:root:Loss: 3.0098 +/- 0.0114
INFO:root:Epoch no. 12 of 500 (samples: 141442)
INFO:root:Loss: 2.9545 +/- 0.0101
INFO:root:Epoch no. 13 of 500 (samples: 141442)
INFO:root:Loss: 2.907 +/- 0.0113
INFO:root:Epoch no. 14 of 500 (samples: 141442)
INFO:root:Loss: 2.866 +/- 0.0113
INFO:root:Epoch no. 15 of 500 (samples: 141442)
INFO:root:Loss: 2.8299 +/- 0.0067
INFO:root:Epoch no. 16 of 500 (samples: 141442)
INFO:root:Loss: 2.7965 +/- 0.0086
INFO:root:Epoch no. 17 of 500 (samples: 141442)
INFO:root:Loss: 2.7695 +/- 0.006
INFO:root:Epoch no. 18 of 500 (samples: 141442)
INFO:root:Loss: 2.7516 +/- 0.0074
INFO:root:Epoch no. 19 of 500 (samples: 141442)
INFO:root:Loss: 2.7343 +/- 0.0047
INFO:root:Epoch no. 20 of 500 (samples: 141442)
INFO:root:Loss: 2.7185 +/- 0.0058
INFO:root:Epoch no. 21 of 500 (samples: 141442)
INFO:root:Loss: 2.7051 +/- 0.0037
INFO:root:Epoch no. 22 of 500 (samples: 141442)
INFO:root:Loss: 2.694 +/- 0.0046
INFO:root:Epoch no. 23 of 500 (samples: 141442)
INFO:root:Loss: 2.6816 +/- 0.0066
INFO:root:Epoch no. 24 of 500 (samples: 141442)
INFO:root:Loss: 2.6759 +/- 0.0072
INFO:root:Epoch no. 25 of 500 (samples: 141442)
INFO:root:Loss: 2.6685 +/- 0.0066
INFO:root:Epoch no. 26 of 500 (samples: 141442)
INFO:root:Loss: 2.6578 +/- 0.0051
INFO:root:Epoch no. 27 of 500 (samples: 141442)
INFO:root:Loss: 2.6536 +/- 0.0067
INFO:root:Epoch no. 28 of 500 (samples: 141442)
INFO:root:Loss: 2.6464 +/- 0.0067
INFO:root:Epoch no. 29 of 500 (samples: 141442)
INFO:root:Loss: 2.6399 +/- 0.0071
INFO:root:Epoch no. 30 of 500 (samples: 141442)
INFO:root:Loss: 2.6334 +/- 0.0063
INFO:root:Epoch no. 31 of 500 (samples: 141442)
INFO:root:Loss: 2.6292 +/- 0.0028
INFO:root:Epoch no. 32 of 500 (samples: 141442)
INFO:root:Loss: 2.621 +/- 0.0067
INFO:root:Epoch no. 33 of 500 (samples: 141442)
INFO:root:Loss: 2.6207 +/- 0.0074
INFO:root:Epoch no. 34 of 500 (samples: 141442)
INFO:root:Loss: 2.6144 +/- 0.0057
INFO:root:Epoch no. 35 of 500 (samples: 141442)
INFO:root:Loss: 2.6092 +/- 0.0083
INFO:root:Epoch no. 36 of 500 (samples: 141442)
INFO:root:Loss: 2.6013 +/- 0.0055
INFO:root:Epoch no. 37 of 500 (samples: 141442)
INFO:root:Loss: 2.5996 +/- 0.0036
INFO:root:Epoch no. 38 of 500 (samples: 141442)
INFO:root:Loss: 2.5951 +/- 0.0043
INFO:root:Epoch no. 39 of 500 (samples: 141442)
INFO:root:Loss: 2.5898 +/- 0.0079
INFO:root:Epoch no. 40 of 500 (samples: 141442)
INFO:root:Loss: 2.5847 +/- 0.0047
INFO:root:Epoch no. 41 of 500 (samples: 141442)
INFO:root:Loss: 2.5814 +/- 0.0076
INFO:root:Epoch no. 42 of 500 (samples: 141442)
INFO:root:Loss: 2.579 +/- 0.0055
INFO:root:Epoch no. 43 of 500 (samples: 141442)
INFO:root:Loss: 2.5761 +/- 0.006
INFO:root:Epoch no. 44 of 500 (samples: 141442)
INFO:root:Loss: 2.5718 +/- 0.0066
INFO:root:Epoch no. 45 of 500 (samples: 141442)
INFO:root:Loss: 2.5694 +/- 0.0065
INFO:root:Epoch no. 46 of 500 (samples: 141442)
INFO:root:Loss: 2.5658 +/- 0.0049
INFO:root:Epoch no. 47 of 500 (samples: 141442)
INFO:root:Loss: 2.5625 +/- 0.005
INFO:root:Epoch no. 48 of 500 (samples: 141442)
INFO:root:Loss: 2.5576 +/- 0.0096
INFO:root:Epoch no. 49 of 500 (samples: 141442)
INFO:root:Loss: 2.5542 +/- 0.0087
INFO:root:Epoch no. 50 of 500 (samples: 141442)
INFO:root:Loss: 2.5518 +/- 0.009
INFO:root:Epoch no. 51 of 500 (samples: 141442)
INFO:root:Loss: 2.5476 +/- 0.0072
INFO:root:Epoch no. 52 of 500 (samples: 141442)
INFO:root:Loss: 2.544 +/- 0.0096
INFO:root:Epoch no. 53 of 500 (samples: 141442)
INFO:root:Loss: 2.5389 +/- 0.0103
INFO:root:Epoch no. 54 of 500 (samples: 141442)
INFO:root:Loss: 2.5389 +/- 0.0068
INFO:root:Epoch no. 55 of 500 (samples: 141442)
INFO:root:Loss: 2.5348 +/- 0.005
INFO:root:Epoch no. 56 of 500 (samples: 141442)
INFO:root:Loss: 2.5318 +/- 0.0064
INFO:root:Epoch no. 57 of 500 (samples: 141442)
INFO:root:Loss: 2.5297 +/- 0.007
INFO:root:Epoch no. 58 of 500 (samples: 141442)
INFO:root:Loss: 2.5251 +/- 0.0082
INFO:root:Epoch no. 59 of 500 (samples: 141442)
INFO:root:Loss: 2.5226 +/- 0.0103
INFO:root:Epoch no. 60 of 500 (samples: 141442)
INFO:root:Loss: 2.5188 +/- 0.0076
INFO:root:Epoch no. 61 of 500 (samples: 141442)
INFO:root:Loss: 2.5169 +/- 0.0084
INFO:root:Epoch no. 62 of 500 (samples: 141442)
INFO:root:Loss: 2.5133 +/- 0.0074
INFO:root:Epoch no. 63 of 500 (samples: 141442)
INFO:root:Loss: 2.5096 +/- 0.0071
INFO:root:Epoch no. 64 of 500 (samples: 141442)
INFO:root:Loss: 2.5087 +/- 0.0071
INFO:root:Epoch no. 65 of 500 (samples: 141442)
INFO:root:Loss: 2.5053 +/- 0.0056
INFO:root:Epoch no. 66 of 500 (samples: 141442)
INFO:root:Loss: 2.5004 +/- 0.0075
INFO:root:Epoch no. 67 of 500 (samples: 141442)
INFO:root:Loss: 2.5012 +/- 0.0039
INFO:root:Epoch no. 68 of 500 (samples: 141442)
INFO:root:Loss: 2.4965 +/- 0.0098
INFO:root:Epoch no. 69 of 500 (samples: 141442)
INFO:root:Loss: 2.4948 +/- 0.0074
INFO:root:Epoch no. 70 of 500 (samples: 141442)
INFO:root:Loss: 2.4897 +/- 0.0039
INFO:root:Epoch no. 71 of 500 (samples: 141442)
INFO:root:Loss: 2.4871 +/- 0.0079
INFO:root:Epoch no. 72 of 500 (samples: 141442)
INFO:root:Loss: 2.4846 +/- 0.0077
INFO:root:Epoch no. 73 of 500 (samples: 141442)
INFO:root:Loss: 2.4831 +/- 0.0074
INFO:root:Epoch no. 74 of 500 (samples: 141442)
INFO:root:Loss: 2.4808 +/- 0.0101
INFO:root:Epoch no. 75 of 500 (samples: 141442)
INFO:root:Loss: 2.4774 +/- 0.0079
INFO:root:Epoch no. 76 of 500 (samples: 141442)
INFO:root:Loss: 2.4745 +/- 0.0066
INFO:root:Epoch no. 77 of 500 (samples: 141442)
INFO:root:Loss: 2.4713 +/- 0.0059
INFO:root:Epoch no. 78 of 500 (samples: 141442)
INFO:root:Loss: 2.4672 +/- 0.0063
INFO:root:Epoch no. 79 of 500 (samples: 141442)
INFO:root:Loss: 2.4663 +/- 0.0096
INFO:root:Epoch no. 80 of 500 (samples: 141442)
INFO:root:Loss: 2.4628 +/- 0.0056
INFO:root:Epoch no. 81 of 500 (samples: 141442)
INFO:root:Loss: 2.4599 +/- 0.0072
INFO:root:Epoch no. 82 of 500 (samples: 141442)
INFO:root:Loss: 2.458 +/- 0.0062
INFO:root:Epoch no. 83 of 500 (samples: 141442)
INFO:root:Loss: 2.4554 +/- 0.0062
INFO:root:Epoch no. 84 of 500 (samples: 141442)
INFO:root:Loss: 2.4498 +/- 0.0067
INFO:root:Epoch no. 85 of 500 (samples: 141442)
INFO:root:Loss: 2.4462 +/- 0.0077
INFO:root:Epoch no. 86 of 500 (samples: 141442)
INFO:root:Loss: 2.4442 +/- 0.0067
INFO:root:Epoch no. 87 of 500 (samples: 141442)
INFO:root:Loss: 2.4442 +/- 0.005
INFO:root:Epoch no. 88 of 500 (samples: 141442)
INFO:root:Loss: 2.4397 +/- 0.0051
INFO:root:Epoch no. 89 of 500 (samples: 141442)
INFO:root:Loss: 2.4377 +/- 0.006
INFO:root:Epoch no. 90 of 500 (samples: 141442)
INFO:root:Loss: 2.4348 +/- 0.007
INFO:root:Epoch no. 91 of 500 (samples: 141442)
INFO:root:Loss: 2.4326 +/- 0.0086
INFO:root:Epoch no. 92 of 500 (samples: 141442)
INFO:root:Loss: 2.4272 +/- 0.0055
INFO:root:Epoch no. 93 of 500 (samples: 141442)
INFO:root:Loss: 2.4252 +/- 0.0076
INFO:root:Epoch no. 94 of 500 (samples: 141442)
INFO:root:Loss: 2.422 +/- 0.0052
INFO:root:Epoch no. 95 of 500 (samples: 141442)
INFO:root:Loss: 2.4201 +/- 0.0047
INFO:root:Epoch no. 96 of 500 (samples: 141442)
INFO:root:Loss: 2.4183 +/- 0.0046
INFO:root:Epoch no. 97 of 500 (samples: 141442)
INFO:root:Loss: 2.4137 +/- 0.0066
INFO:root:Epoch no. 98 of 500 (samples: 141442)
INFO:root:Loss: 2.4118 +/- 0.005
INFO:root:Epoch no. 99 of 500 (samples: 141442)
INFO:root:Loss: 2.4094 +/- 0.0074
INFO:root:Epoch no. 100 of 500 (samples: 141442)
INFO:root:Loss: 2.4061 +/- 0.011
INFO:root:Epoch no. 101 of 500 (samples: 141442)
INFO:root:Loss: 2.4051 +/- 0.0066
INFO:root:Epoch no. 102 of 500 (samples: 141442)
INFO:root:Loss: 2.4025 +/- 0.0059
INFO:root:Epoch no. 103 of 500 (samples: 141442)
INFO:root:Loss: 2.3992 +/- 0.0054
INFO:root:Epoch no. 104 of 500 (samples: 141442)
INFO:root:Loss: 2.3939 +/- 0.0052
INFO:root:Epoch no. 105 of 500 (samples: 141442)
INFO:root:Loss: 2.3938 +/- 0.0082
INFO:root:Epoch no. 106 of 500 (samples: 141442)
INFO:root:Loss: 2.3887 +/- 0.0078
INFO:root:Epoch no. 107 of 500 (samples: 141442)
INFO:root:Loss: 2.3886 +/- 0.0076
INFO:root:Epoch no. 108 of 500 (samples: 141442)
INFO:root:Loss: 2.3863 +/- 0.0082
INFO:root:Epoch no. 109 of 500 (samples: 141442)
INFO:root:Loss: 2.3805 +/- 0.0107
INFO:root:Epoch no. 110 of 500 (samples: 141442)
INFO:root:Loss: 2.3796 +/- 0.0053
INFO:root:Epoch no. 111 of 500 (samples: 141442)
INFO:root:Loss: 2.377 +/- 0.0066
INFO:root:Epoch no. 112 of 500 (samples: 141442)
INFO:root:Loss: 2.3731 +/- 0.0069
INFO:root:Epoch no. 113 of 500 (samples: 141442)
INFO:root:Loss: 2.3694 +/- 0.008
INFO:root:Epoch no. 114 of 500 (samples: 141442)
INFO:root:Loss: 2.368 +/- 0.0101
INFO:root:Epoch no. 115 of 500 (samples: 141442)
INFO:root:Loss: 2.3652 +/- 0.0051
INFO:root:Epoch no. 116 of 500 (samples: 141442)
INFO:root:Loss: 2.3619 +/- 0.0059
INFO:root:Epoch no. 117 of 500 (samples: 141442)
INFO:root:Loss: 2.3608 +/- 0.0055
INFO:root:Epoch no. 118 of 500 (samples: 141442)
INFO:root:Loss: 2.3584 +/- 0.0048
INFO:root:Epoch no. 119 of 500 (samples: 141442)
INFO:root:Loss: 2.3533 +/- 0.0061
INFO:root:Epoch no. 120 of 500 (samples: 141442)
INFO:root:Loss: 2.3532 +/- 0.0068
INFO:root:Epoch no. 121 of 500 (samples: 141442)
INFO:root:Loss: 2.35 +/- 0.0116
INFO:root:Epoch no. 122 of 500 (samples: 141442)
INFO:root:Loss: 2.3485 +/- 0.0078
INFO:root:Epoch no. 123 of 500 (samples: 141442)
INFO:root:Loss: 2.3456 +/- 0.0067
INFO:root:Epoch no. 124 of 500 (samples: 141442)
INFO:root:Loss: 2.3431 +/- 0.0034
INFO:root:Epoch no. 125 of 500 (samples: 141442)
INFO:root:Loss: 2.3394 +/- 0.0086
INFO:root:Epoch no. 126 of 500 (samples: 141442)
INFO:root:Loss: 2.3362 +/- 0.0067
INFO:root:Epoch no. 127 of 500 (samples: 141442)
INFO:root:Loss: 2.3367 +/- 0.0053
INFO:root:Epoch no. 128 of 500 (samples: 141442)
INFO:root:Loss: 2.3336 +/- 0.0076
INFO:root:Epoch no. 129 of 500 (samples: 141442)
INFO:root:Loss: 2.3314 +/- 0.0069
INFO:root:Epoch no. 130 of 500 (samples: 141442)
INFO:root:Loss: 2.3291 +/- 0.0086
INFO:root:Epoch no. 131 of 500 (samples: 141442)
INFO:root:Loss: 2.3251 +/- 0.0073
INFO:root:Epoch no. 132 of 500 (samples: 141442)
INFO:root:Loss: 2.3223 +/- 0.0078
INFO:root:Epoch no. 133 of 500 (samples: 141442)
INFO:root:Loss: 2.3214 +/- 0.0084
INFO:root:Epoch no. 134 of 500 (samples: 141442)
INFO:root:Loss: 2.3196 +/- 0.0082
INFO:root:Epoch no. 135 of 500 (samples: 141442)
INFO:root:Loss: 2.3159 +/- 0.0074
INFO:root:Epoch no. 136 of 500 (samples: 141442)
INFO:root:Loss: 2.3128 +/- 0.0086
INFO:root:Epoch no. 137 of 500 (samples: 141442)
INFO:root:Loss: 2.3132 +/- 0.0092
INFO:root:Epoch no. 138 of 500 (samples: 141442)
INFO:root:Loss: 2.3093 +/- 0.009
INFO:root:Epoch no. 139 of 500 (samples: 141442)
INFO:root:Loss: 2.3075 +/- 0.0058
INFO:root:Epoch no. 140 of 500 (samples: 141442)
INFO:root:Loss: 2.304 +/- 0.0085
INFO:root:Epoch no. 141 of 500 (samples: 141442)
INFO:root:Loss: 2.3013 +/- 0.005
INFO:root:Epoch no. 142 of 500 (samples: 141442)
INFO:root:Loss: 2.2976 +/- 0.0057
INFO:root:Epoch no. 143 of 500 (samples: 141442)
INFO:root:Loss: 2.2958 +/- 0.0057
INFO:root:Epoch no. 144 of 500 (samples: 141442)
INFO:root:Loss: 2.2943 +/- 0.0085
INFO:root:Epoch no. 145 of 500 (samples: 141442)
INFO:root:Loss: 2.293 +/- 0.007
INFO:root:Epoch no. 146 of 500 (samples: 141442)
INFO:root:Loss: 2.2932 +/- 0.005
INFO:root:Epoch no. 147 of 500 (samples: 141442)
INFO:root:Loss: 2.2901 +/- 0.0072
INFO:root:Epoch no. 148 of 500 (samples: 141442)
INFO:root:Loss: 2.2855 +/- 0.0068
INFO:root:Epoch no. 149 of 500 (samples: 141442)
INFO:root:Loss: 2.2832 +/- 0.0092
INFO:root:Epoch no. 150 of 500 (samples: 141442)
INFO:root:Loss: 2.2839 +/- 0.0084
INFO:root:Epoch no. 151 of 500 (samples: 141442)
INFO:root:Loss: 2.2782 +/- 0.0076
INFO:root:Epoch no. 152 of 500 (samples: 141442)
INFO:root:Loss: 2.2777 +/- 0.0047
INFO:root:Epoch no. 153 of 500 (samples: 141442)
INFO:root:Loss: 2.2729 +/- 0.0115
INFO:root:Epoch no. 154 of 500 (samples: 141442)
INFO:root:Loss: 2.2732 +/- 0.0083
INFO:root:Epoch no. 155 of 500 (samples: 141442)
INFO:root:Loss: 2.2708 +/- 0.0068
INFO:root:Epoch no. 156 of 500 (samples: 141442)
Traceback (most recent call last):
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: alloc failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./bin/hyper-cli.py", line 324, in <module>
    main(sys.argv[1:])
  File "./bin/hyper-cli.py", line 293, in main
    visualize=is_visualize)
  File "/home/insight/.local/lib/python3.4/site-packages/hyper-0.0.1-py3.4.egg/hyper/learning/core.py", line 185, in pairwise_training
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/models.py", line 409, in fit
    sample_weight=sample_weight)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 1052, in fit
    callback_metrics=callback_metrics)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 790, in _fit_loop
    outs = f(ins_batch)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/backend/theano_backend.py", line 518, in __call__
    return self.function(*inputs)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 871, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/usr/local/lib/python3.4/dist-packages/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/usr/local/lib/python3.4/dist-packages/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: alloc failed
Apply node that caused the error: Alloc(TensorConstant{(1, 1, 1) of 0.0}, Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}[(0, 2)].0, Elemwise{Composite{Switch(EQ(i0, i1), ((i2 * i3 * i0) // (-(i3 * i0 * i2))), i0)}}.0, Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}.0)
Toposort index: 41
Inputs types: [TensorType(float32, (True, True, True)), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]
Inputs shapes: [(1, 1, 1), (), (), ()]
Inputs strides: [(4, 4, 4), (), (), ()]
Inputs values: [array([[[ 0.]]], dtype=float32), array(42435), array(1), array(2500)]
Outputs clients: [[IncSubtensor{InplaceInc;::, int64, ::}(Alloc.0, Reshape{2}.0, Constant{0})]]

Backtrace when the node is created(use Theano flag traceback.limit=N to make it longer):
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1113, in access_term_cache
    input_grads = node.op.grad(inputs, new_output_grads)

HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
