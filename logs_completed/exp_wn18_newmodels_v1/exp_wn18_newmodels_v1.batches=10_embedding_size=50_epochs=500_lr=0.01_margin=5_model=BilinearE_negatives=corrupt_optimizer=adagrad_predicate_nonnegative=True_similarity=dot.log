Using Theano backend.
INFO:root:Acquiring data/wn18/wordnet-mlj12-train.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-valid.txt ..
INFO:root:Acquiring data/wn18/wordnet-mlj12-test.txt ..
INFO:root:Samples: 141442, no. batches: 10 -> batch size: 14145
INFO:root:Epoch no. 1 of 500 (samples: 141442)
INFO:root:Loss: 3.33 +/- 0.0027
INFO:root:Epoch no. 2 of 500 (samples: 141442)
INFO:root:Loss: 3.3013 +/- 0.0086
INFO:root:Epoch no. 3 of 500 (samples: 141442)
INFO:root:Loss: 3.2408 +/- 0.0214
INFO:root:Epoch no. 4 of 500 (samples: 141442)
INFO:root:Loss: 3.1139 +/- 0.0428
INFO:root:Epoch no. 5 of 500 (samples: 141442)
INFO:root:Loss: 2.9042 +/- 0.0591
INFO:root:Epoch no. 6 of 500 (samples: 141442)
INFO:root:Loss: 2.65 +/- 0.0601
INFO:root:Epoch no. 7 of 500 (samples: 141442)
INFO:root:Loss: 2.448 +/- 0.0346
INFO:root:Epoch no. 8 of 500 (samples: 141442)
INFO:root:Loss: 2.3148 +/- 0.0174
INFO:root:Epoch no. 9 of 500 (samples: 141442)
INFO:root:Loss: 2.2275 +/- 0.0191
INFO:root:Epoch no. 10 of 500 (samples: 141442)
INFO:root:Loss: 2.168 +/- 0.0137
INFO:root:Epoch no. 11 of 500 (samples: 141442)
INFO:root:Loss: 2.1231 +/- 0.0159
INFO:root:Epoch no. 12 of 500 (samples: 141442)
INFO:root:Loss: 2.0876 +/- 0.0098
INFO:root:Epoch no. 13 of 500 (samples: 141442)
INFO:root:Loss: 2.0619 +/- 0.0199
INFO:root:Epoch no. 14 of 500 (samples: 141442)
INFO:root:Loss: 2.0461 +/- 0.0127
INFO:root:Epoch no. 15 of 500 (samples: 141442)
INFO:root:Loss: 2.0249 +/- 0.0151
INFO:root:Epoch no. 16 of 500 (samples: 141442)
INFO:root:Loss: 2.0103 +/- 0.0122
INFO:root:Epoch no. 17 of 500 (samples: 141442)
INFO:root:Loss: 1.9884 +/- 0.0105
INFO:root:Epoch no. 18 of 500 (samples: 141442)
INFO:root:Loss: 1.979 +/- 0.0154
INFO:root:Epoch no. 19 of 500 (samples: 141442)
INFO:root:Loss: 1.97 +/- 0.0119
INFO:root:Epoch no. 20 of 500 (samples: 141442)
INFO:root:Loss: 1.9523 +/- 0.0163
INFO:root:Epoch no. 21 of 500 (samples: 141442)
INFO:root:Loss: 1.9466 +/- 0.0094
INFO:root:Epoch no. 22 of 500 (samples: 141442)
INFO:root:Loss: 1.9381 +/- 0.0069
INFO:root:Epoch no. 23 of 500 (samples: 141442)
INFO:root:Loss: 1.9202 +/- 0.0144
INFO:root:Epoch no. 24 of 500 (samples: 141442)
INFO:root:Loss: 1.9184 +/- 0.0128
INFO:root:Epoch no. 25 of 500 (samples: 141442)
INFO:root:Loss: 1.9088 +/- 0.0109
INFO:root:Epoch no. 26 of 500 (samples: 141442)
INFO:root:Loss: 1.8941 +/- 0.0131
INFO:root:Epoch no. 27 of 500 (samples: 141442)
INFO:root:Loss: 1.8911 +/- 0.0111
INFO:root:Epoch no. 28 of 500 (samples: 141442)
INFO:root:Loss: 1.8849 +/- 0.0082
INFO:root:Epoch no. 29 of 500 (samples: 141442)
INFO:root:Loss: 1.8775 +/- 0.01
INFO:root:Epoch no. 30 of 500 (samples: 141442)
INFO:root:Loss: 1.8721 +/- 0.0176
INFO:root:Epoch no. 31 of 500 (samples: 141442)
INFO:root:Loss: 1.8657 +/- 0.01
INFO:root:Epoch no. 32 of 500 (samples: 141442)
INFO:root:Loss: 1.8511 +/- 0.0117
INFO:root:Epoch no. 33 of 500 (samples: 141442)
INFO:root:Loss: 1.8556 +/- 0.0129
INFO:root:Epoch no. 34 of 500 (samples: 141442)
INFO:root:Loss: 1.8448 +/- 0.011
INFO:root:Epoch no. 35 of 500 (samples: 141442)
INFO:root:Loss: 1.8426 +/- 0.0112
INFO:root:Epoch no. 36 of 500 (samples: 141442)
INFO:root:Loss: 1.8361 +/- 0.0185
INFO:root:Epoch no. 37 of 500 (samples: 141442)
INFO:root:Loss: 1.8334 +/- 0.0122
INFO:root:Epoch no. 38 of 500 (samples: 141442)
INFO:root:Loss: 1.8271 +/- 0.0183
INFO:root:Epoch no. 39 of 500 (samples: 141442)
INFO:root:Loss: 1.8218 +/- 0.0101
INFO:root:Epoch no. 40 of 500 (samples: 141442)
INFO:root:Loss: 1.8168 +/- 0.0124
INFO:root:Epoch no. 41 of 500 (samples: 141442)
INFO:root:Loss: 1.8147 +/- 0.0165
INFO:root:Epoch no. 42 of 500 (samples: 141442)
INFO:root:Loss: 1.8048 +/- 0.0105
INFO:root:Epoch no. 43 of 500 (samples: 141442)
INFO:root:Loss: 1.8019 +/- 0.0139
INFO:root:Epoch no. 44 of 500 (samples: 141442)
INFO:root:Loss: 1.8014 +/- 0.0126
INFO:root:Epoch no. 45 of 500 (samples: 141442)
INFO:root:Loss: 1.7945 +/- 0.0084
INFO:root:Epoch no. 46 of 500 (samples: 141442)
INFO:root:Loss: 1.7977 +/- 0.0206
INFO:root:Epoch no. 47 of 500 (samples: 141442)
INFO:root:Loss: 1.7811 +/- 0.0182
INFO:root:Epoch no. 48 of 500 (samples: 141442)
INFO:root:Loss: 1.7758 +/- 0.0169
INFO:root:Epoch no. 49 of 500 (samples: 141442)
INFO:root:Loss: 1.7714 +/- 0.0094
INFO:root:Epoch no. 50 of 500 (samples: 141442)
INFO:root:Loss: 1.7756 +/- 0.0109
INFO:root:Epoch no. 51 of 500 (samples: 141442)
INFO:root:Loss: 1.77 +/- 0.0144
INFO:root:Epoch no. 52 of 500 (samples: 141442)
INFO:root:Loss: 1.7615 +/- 0.0119
INFO:root:Epoch no. 53 of 500 (samples: 141442)
INFO:root:Loss: 1.758 +/- 0.0165
INFO:root:Epoch no. 54 of 500 (samples: 141442)
INFO:root:Loss: 1.7532 +/- 0.0094
INFO:root:Epoch no. 55 of 500 (samples: 141442)
INFO:root:Loss: 1.7557 +/- 0.0089
INFO:root:Epoch no. 56 of 500 (samples: 141442)
INFO:root:Loss: 1.747 +/- 0.0138
INFO:root:Epoch no. 57 of 500 (samples: 141442)
INFO:root:Loss: 1.7443 +/- 0.0166
INFO:root:Epoch no. 58 of 500 (samples: 141442)
INFO:root:Loss: 1.7415 +/- 0.0124
INFO:root:Epoch no. 59 of 500 (samples: 141442)
INFO:root:Loss: 1.7335 +/- 0.0108
INFO:root:Epoch no. 60 of 500 (samples: 141442)
INFO:root:Loss: 1.7315 +/- 0.0146
INFO:root:Epoch no. 61 of 500 (samples: 141442)
INFO:root:Loss: 1.7284 +/- 0.0109
INFO:root:Epoch no. 62 of 500 (samples: 141442)
INFO:root:Loss: 1.7253 +/- 0.0068
INFO:root:Epoch no. 63 of 500 (samples: 141442)
INFO:root:Loss: 1.7168 +/- 0.0123
INFO:root:Epoch no. 64 of 500 (samples: 141442)
INFO:root:Loss: 1.7172 +/- 0.0111
INFO:root:Epoch no. 65 of 500 (samples: 141442)
INFO:root:Loss: 1.7162 +/- 0.0156
INFO:root:Epoch no. 66 of 500 (samples: 141442)
INFO:root:Loss: 1.7079 +/- 0.0119
INFO:root:Epoch no. 67 of 500 (samples: 141442)
INFO:root:Loss: 1.7048 +/- 0.012
INFO:root:Epoch no. 68 of 500 (samples: 141442)
INFO:root:Loss: 1.7036 +/- 0.0152
INFO:root:Epoch no. 69 of 500 (samples: 141442)
INFO:root:Loss: 1.6968 +/- 0.0081
INFO:root:Epoch no. 70 of 500 (samples: 141442)
INFO:root:Loss: 1.696 +/- 0.015
INFO:root:Epoch no. 71 of 500 (samples: 141442)
INFO:root:Loss: 1.6913 +/- 0.0149
INFO:root:Epoch no. 72 of 500 (samples: 141442)
INFO:root:Loss: 1.6935 +/- 0.01
INFO:root:Epoch no. 73 of 500 (samples: 141442)
INFO:root:Loss: 1.6794 +/- 0.015
INFO:root:Epoch no. 74 of 500 (samples: 141442)
INFO:root:Loss: 1.6801 +/- 0.0171
INFO:root:Epoch no. 75 of 500 (samples: 141442)
INFO:root:Loss: 1.6785 +/- 0.0119
INFO:root:Epoch no. 76 of 500 (samples: 141442)
INFO:root:Loss: 1.6749 +/- 0.0146
INFO:root:Epoch no. 77 of 500 (samples: 141442)
INFO:root:Loss: 1.6655 +/- 0.0132
INFO:root:Epoch no. 78 of 500 (samples: 141442)
INFO:root:Loss: 1.6669 +/- 0.0077
INFO:root:Epoch no. 79 of 500 (samples: 141442)
INFO:root:Loss: 1.6648 +/- 0.0147
INFO:root:Epoch no. 80 of 500 (samples: 141442)
INFO:root:Loss: 1.6558 +/- 0.0124
INFO:root:Epoch no. 81 of 500 (samples: 141442)
INFO:root:Loss: 1.6546 +/- 0.0159
INFO:root:Epoch no. 82 of 500 (samples: 141442)
INFO:root:Loss: 1.6555 +/- 0.011
INFO:root:Epoch no. 83 of 500 (samples: 141442)
INFO:root:Loss: 1.6473 +/- 0.0147
INFO:root:Epoch no. 84 of 500 (samples: 141442)
INFO:root:Loss: 1.643 +/- 0.0087
INFO:root:Epoch no. 85 of 500 (samples: 141442)
INFO:root:Loss: 1.6406 +/- 0.0141
INFO:root:Epoch no. 86 of 500 (samples: 141442)
INFO:root:Loss: 1.6407 +/- 0.0166
INFO:root:Epoch no. 87 of 500 (samples: 141442)
INFO:root:Loss: 1.6336 +/- 0.0105
INFO:root:Epoch no. 88 of 500 (samples: 141442)
INFO:root:Loss: 1.6315 +/- 0.0086
INFO:root:Epoch no. 89 of 500 (samples: 141442)
INFO:root:Loss: 1.6223 +/- 0.0161
INFO:root:Epoch no. 90 of 500 (samples: 141442)
INFO:root:Loss: 1.6211 +/- 0.0119
INFO:root:Epoch no. 91 of 500 (samples: 141442)
INFO:root:Loss: 1.6219 +/- 0.0126
INFO:root:Epoch no. 92 of 500 (samples: 141442)
INFO:root:Loss: 1.6211 +/- 0.0164
INFO:root:Epoch no. 93 of 500 (samples: 141442)
INFO:root:Loss: 1.6154 +/- 0.0176
INFO:root:Epoch no. 94 of 500 (samples: 141442)
INFO:root:Loss: 1.6052 +/- 0.013
INFO:root:Epoch no. 95 of 500 (samples: 141442)
INFO:root:Loss: 1.607 +/- 0.0165
INFO:root:Epoch no. 96 of 500 (samples: 141442)
INFO:root:Loss: 1.6047 +/- 0.0116
INFO:root:Epoch no. 97 of 500 (samples: 141442)
INFO:root:Loss: 1.5987 +/- 0.0132
INFO:root:Epoch no. 98 of 500 (samples: 141442)
INFO:root:Loss: 1.5974 +/- 0.0106
INFO:root:Epoch no. 99 of 500 (samples: 141442)
INFO:root:Loss: 1.5916 +/- 0.0102
INFO:root:Epoch no. 100 of 500 (samples: 141442)
INFO:root:Loss: 1.5887 +/- 0.0123
INFO:root:Epoch no. 101 of 500 (samples: 141442)
INFO:root:Loss: 1.5816 +/- 0.0115
INFO:root:Epoch no. 102 of 500 (samples: 141442)
INFO:root:Loss: 1.5759 +/- 0.0113
INFO:root:Epoch no. 103 of 500 (samples: 141442)
INFO:root:Loss: 1.5795 +/- 0.0118
INFO:root:Epoch no. 104 of 500 (samples: 141442)
INFO:root:Loss: 1.5711 +/- 0.0143
INFO:root:Epoch no. 105 of 500 (samples: 141442)
INFO:root:Loss: 1.5715 +/- 0.0145
INFO:root:Epoch no. 106 of 500 (samples: 141442)
INFO:root:Loss: 1.5672 +/- 0.0113
INFO:root:Epoch no. 107 of 500 (samples: 141442)
INFO:root:Loss: 1.5586 +/- 0.0086
INFO:root:Epoch no. 108 of 500 (samples: 141442)
INFO:root:Loss: 1.5681 +/- 0.0191
INFO:root:Epoch no. 109 of 500 (samples: 141442)
INFO:root:Loss: 1.5548 +/- 0.007
INFO:root:Epoch no. 110 of 500 (samples: 141442)
INFO:root:Loss: 1.555 +/- 0.0137
INFO:root:Epoch no. 111 of 500 (samples: 141442)
INFO:root:Loss: 1.548 +/- 0.0123
INFO:root:Epoch no. 112 of 500 (samples: 141442)
INFO:root:Loss: 1.5437 +/- 0.0185
INFO:root:Epoch no. 113 of 500 (samples: 141442)
INFO:root:Loss: 1.5429 +/- 0.0185
INFO:root:Epoch no. 114 of 500 (samples: 141442)
INFO:root:Loss: 1.5348 +/- 0.0135
INFO:root:Epoch no. 115 of 500 (samples: 141442)
INFO:root:Loss: 1.5408 +/- 0.0124
INFO:root:Epoch no. 116 of 500 (samples: 141442)
INFO:root:Loss: 1.5373 +/- 0.0114
INFO:root:Epoch no. 117 of 500 (samples: 141442)
INFO:root:Loss: 1.5327 +/- 0.0123
INFO:root:Epoch no. 118 of 500 (samples: 141442)
INFO:root:Loss: 1.5265 +/- 0.0115
INFO:root:Epoch no. 119 of 500 (samples: 141442)
INFO:root:Loss: 1.516 +/- 0.0119
INFO:root:Epoch no. 120 of 500 (samples: 141442)
INFO:root:Loss: 1.5183 +/- 0.0117
INFO:root:Epoch no. 121 of 500 (samples: 141442)
INFO:root:Loss: 1.5145 +/- 0.0142
INFO:root:Epoch no. 122 of 500 (samples: 141442)
INFO:root:Loss: 1.5066 +/- 0.0175
INFO:root:Epoch no. 123 of 500 (samples: 141442)
INFO:root:Loss: 1.5123 +/- 0.0176
INFO:root:Epoch no. 124 of 500 (samples: 141442)
INFO:root:Loss: 1.5049 +/- 0.0108
INFO:root:Epoch no. 125 of 500 (samples: 141442)
INFO:root:Loss: 1.51 +/- 0.0114
INFO:root:Epoch no. 126 of 500 (samples: 141442)
INFO:root:Loss: 1.497 +/- 0.0132
INFO:root:Epoch no. 127 of 500 (samples: 141442)
INFO:root:Loss: 1.4993 +/- 0.0101
INFO:root:Epoch no. 128 of 500 (samples: 141442)
INFO:root:Loss: 1.4924 +/- 0.0128
INFO:root:Epoch no. 129 of 500 (samples: 141442)
INFO:root:Loss: 1.486 +/- 0.0155
INFO:root:Epoch no. 130 of 500 (samples: 141442)
INFO:root:Loss: 1.4857 +/- 0.0148
INFO:root:Epoch no. 131 of 500 (samples: 141442)
INFO:root:Loss: 1.4713 +/- 0.0068
INFO:root:Epoch no. 132 of 500 (samples: 141442)
INFO:root:Loss: 1.4753 +/- 0.0153
INFO:root:Epoch no. 133 of 500 (samples: 141442)
INFO:root:Loss: 1.4671 +/- 0.0124
INFO:root:Epoch no. 134 of 500 (samples: 141442)
INFO:root:Loss: 1.4644 +/- 0.0121
INFO:root:Epoch no. 135 of 500 (samples: 141442)
INFO:root:Loss: 1.4648 +/- 0.0089
INFO:root:Epoch no. 136 of 500 (samples: 141442)
INFO:root:Loss: 1.458 +/- 0.0135
INFO:root:Epoch no. 137 of 500 (samples: 141442)
INFO:root:Loss: 1.4581 +/- 0.0235
INFO:root:Epoch no. 138 of 500 (samples: 141442)
INFO:root:Loss: 1.4562 +/- 0.013
INFO:root:Epoch no. 139 of 500 (samples: 141442)
INFO:root:Loss: 1.4548 +/- 0.0158
INFO:root:Epoch no. 140 of 500 (samples: 141442)
INFO:root:Loss: 1.4465 +/- 0.0189
INFO:root:Epoch no. 141 of 500 (samples: 141442)
INFO:root:Loss: 1.4442 +/- 0.0078
INFO:root:Epoch no. 142 of 500 (samples: 141442)
INFO:root:Loss: 1.444 +/- 0.0098
INFO:root:Epoch no. 143 of 500 (samples: 141442)
INFO:root:Loss: 1.4362 +/- 0.0109
INFO:root:Epoch no. 144 of 500 (samples: 141442)
INFO:root:Loss: 1.4334 +/- 0.0111
INFO:root:Epoch no. 145 of 500 (samples: 141442)
INFO:root:Loss: 1.4305 +/- 0.0121
INFO:root:Epoch no. 146 of 500 (samples: 141442)
INFO:root:Loss: 1.4246 +/- 0.0074
INFO:root:Epoch no. 147 of 500 (samples: 141442)
INFO:root:Loss: 1.4241 +/- 0.0115
INFO:root:Epoch no. 148 of 500 (samples: 141442)
INFO:root:Loss: 1.4161 +/- 0.0141
INFO:root:Epoch no. 149 of 500 (samples: 141442)
INFO:root:Loss: 1.4156 +/- 0.0105
INFO:root:Epoch no. 150 of 500 (samples: 141442)
INFO:root:Loss: 1.4107 +/- 0.0165
INFO:root:Epoch no. 151 of 500 (samples: 141442)
INFO:root:Loss: 1.4111 +/- 0.0164
INFO:root:Epoch no. 152 of 500 (samples: 141442)
INFO:root:Loss: 1.4066 +/- 0.0164
INFO:root:Epoch no. 153 of 500 (samples: 141442)
INFO:root:Loss: 1.4038 +/- 0.0083
INFO:root:Epoch no. 154 of 500 (samples: 141442)
INFO:root:Loss: 1.3991 +/- 0.0111
INFO:root:Epoch no. 155 of 500 (samples: 141442)
INFO:root:Loss: 1.3974 +/- 0.0123
INFO:root:Epoch no. 156 of 500 (samples: 141442)
INFO:root:Loss: 1.3928 +/- 0.0152
INFO:root:Epoch no. 157 of 500 (samples: 141442)
INFO:root:Loss: 1.3924 +/- 0.0168
INFO:root:Epoch no. 158 of 500 (samples: 141442)
INFO:root:Loss: 1.384 +/- 0.0127
INFO:root:Epoch no. 159 of 500 (samples: 141442)
INFO:root:Loss: 1.3846 +/- 0.0167
INFO:root:Epoch no. 160 of 500 (samples: 141442)
INFO:root:Loss: 1.384 +/- 0.0151
INFO:root:Epoch no. 161 of 500 (samples: 141442)
INFO:root:Loss: 1.3677 +/- 0.0123
INFO:root:Epoch no. 162 of 500 (samples: 141442)
INFO:root:Loss: 1.3668 +/- 0.0144
INFO:root:Epoch no. 163 of 500 (samples: 141442)
INFO:root:Loss: 1.3731 +/- 0.0087
INFO:root:Epoch no. 164 of 500 (samples: 141442)
INFO:root:Loss: 1.3597 +/- 0.0081
INFO:root:Epoch no. 165 of 500 (samples: 141442)
INFO:root:Loss: 1.3635 +/- 0.0103
INFO:root:Epoch no. 166 of 500 (samples: 141442)
INFO:root:Loss: 1.3592 +/- 0.014
INFO:root:Epoch no. 167 of 500 (samples: 141442)
INFO:root:Loss: 1.3631 +/- 0.0085
INFO:root:Epoch no. 168 of 500 (samples: 141442)
INFO:root:Loss: 1.3545 +/- 0.0113
INFO:root:Epoch no. 169 of 500 (samples: 141442)
INFO:root:Loss: 1.3546 +/- 0.0126
INFO:root:Epoch no. 170 of 500 (samples: 141442)
INFO:root:Loss: 1.3539 +/- 0.0103
INFO:root:Epoch no. 171 of 500 (samples: 141442)
INFO:root:Loss: 1.3422 +/- 0.0156
INFO:root:Epoch no. 172 of 500 (samples: 141442)
INFO:root:Loss: 1.3433 +/- 0.0102
INFO:root:Epoch no. 173 of 500 (samples: 141442)
INFO:root:Loss: 1.347 +/- 0.0094
INFO:root:Epoch no. 174 of 500 (samples: 141442)
INFO:root:Loss: 1.3371 +/- 0.012
INFO:root:Epoch no. 175 of 500 (samples: 141442)
INFO:root:Loss: 1.3323 +/- 0.0077
INFO:root:Epoch no. 176 of 500 (samples: 141442)
INFO:root:Loss: 1.3276 +/- 0.0108
INFO:root:Epoch no. 177 of 500 (samples: 141442)
INFO:root:Loss: 1.3274 +/- 0.0056
INFO:root:Epoch no. 178 of 500 (samples: 141442)
INFO:root:Loss: 1.3267 +/- 0.0152
INFO:root:Epoch no. 179 of 500 (samples: 141442)
INFO:root:Loss: 1.3173 +/- 0.0125
INFO:root:Epoch no. 180 of 500 (samples: 141442)
INFO:root:Loss: 1.3171 +/- 0.01
INFO:root:Epoch no. 181 of 500 (samples: 141442)
INFO:root:Loss: 1.3125 +/- 0.0121
INFO:root:Epoch no. 182 of 500 (samples: 141442)
INFO:root:Loss: 1.3097 +/- 0.0099
INFO:root:Epoch no. 183 of 500 (samples: 141442)
INFO:root:Loss: 1.3058 +/- 0.0108
INFO:root:Epoch no. 184 of 500 (samples: 141442)
INFO:root:Loss: 1.303 +/- 0.0131
INFO:root:Epoch no. 185 of 500 (samples: 141442)
INFO:root:Loss: 1.2994 +/- 0.0123
INFO:root:Epoch no. 186 of 500 (samples: 141442)
Traceback (most recent call last):
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: alloc failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "./bin/hyper-cli.py", line 324, in <module>
    main(sys.argv[1:])
  File "./bin/hyper-cli.py", line 293, in main
    visualize=is_visualize)
  File "/home/insight/.local/lib/python3.4/site-packages/hyper-0.0.1-py3.4.egg/hyper/learning/core.py", line 185, in pairwise_training
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/models.py", line 409, in fit
    sample_weight=sample_weight)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 1052, in fit
    callback_metrics=callback_metrics)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/engine/training.py", line 790, in _fit_loop
    outs = f(ins_batch)
  File "/home/insight/.local/lib/python3.4/site-packages/Keras-1.0.3-py3.4.egg/keras/backend/theano_backend.py", line 518, in __call__
    return self.function(*inputs)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 871, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/usr/local/lib/python3.4/dist-packages/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/usr/local/lib/python3.4/dist-packages/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
MemoryError: alloc failed
Apply node that caused the error: Alloc(TensorConstant{(1, 1, 1) of 0.0}, Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}[(0, 2)].0, Elemwise{Composite{Switch(EQ(i0, i1), ((i2 * i3 * i0) // (-(i3 * i0 * i2))), i0)}}.0, Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}.0)
Toposort index: 41
Inputs types: [TensorType(float32, (True, True, True)), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]
Inputs shapes: [(1, 1, 1), (), (), ()]
Inputs strides: [(4, 4, 4), (), (), ()]
Inputs values: [array([[[ 0.]]], dtype=float32), array(42435), array(1), array(2500)]
Outputs clients: [[IncSubtensor{InplaceInc;::, int64, ::}(Alloc.0, Reshape{2}.0, Constant{0})]]

Backtrace when the node is created(use Theano flag traceback.limit=N to make it longer):
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 973, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1279, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/usr/local/lib/python3.4/dist-packages/theano/gradient.py", line 1113, in access_term_cache
    input_grads = node.op.grad(inputs, new_output_grads)

HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
